{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QwO2gAgiJS2",
        "outputId": "acec0463-c03f-4359-9680-6ebbc99e4267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.18\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Installing pip packages**\n",
        "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
        "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
        "#@markdown - Push-T Env: gym, pygame, pymunk & shapely\n",
        "!python --version\n",
        "# !pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.18.2 \\\n",
        "# scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
        "# pygame==2.1.2 pymunk==6.2.1 gym==0.26.2 shapely==1.8.4 \\\n",
        "# &> /dev/null # mute output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "49d5ffd9eb81491f95df70e66c0c94e4",
            "9a3d8fbcc3e548e1a56d25c10b321e6a",
            "6cf470f12c174e94b0b058cfba9b5fb2",
            "9b6de90b3dbc4877966b2d5233189866",
            "4b3aeac3e8e74b8f8dab0dc9ad0b7c94",
            "ac6375a33ca84241b50638edb6321112",
            "583a530a9c0442f2b762b281caa4836d",
            "c7d2c3f8b3714a0d911a132c74589ce1",
            "03bbeb04a4c44206b1671e69864e69c7",
            "b2331069d969430b87c47e6f11fb2a9e",
            "c8c282ba14c14da28ba421abc7119cf9"
          ]
        },
        "id": "VrX4VTl5pYNq",
        "outputId": "1a01b12b-0e61-4f38-bb81-f72591e267da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sfchen/miniforge3/envs/robodiff/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.1.2 (SDL 2.0.16, Python 3.9.18)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Imports**\n",
        "# diffusion policy import\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import zarr\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "import shapely.geometry as sg\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "import gdown\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "L5E-nR6ornyg"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Environment**\n",
        "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
        "#@markdown\n",
        "#@markdown **Goal**: push the gray T-block into the green area.\n",
        "#@markdown\n",
        "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
        "\n",
        "\n",
        "positive_y_is_up: bool = False\n",
        "\"\"\"Make increasing values of y point upwards.\n",
        "\n",
        "When True::\n",
        "\n",
        "    y\n",
        "    ^\n",
        "    |      . (3, 3)\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    +------ > x\n",
        "\n",
        "When False::\n",
        "\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
        "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
        "    local coordinates.\n",
        "\n",
        "    Note that in case positive_y_is_up is False, this function wont actually do\n",
        "    anything except converting the point to integers.\n",
        "    \"\"\"\n",
        "    if positive_y_is_up:\n",
        "        return round(p[0]), surface.get_height() - round(p[1])\n",
        "    else:\n",
        "        return round(p[0]), round(p[1])\n",
        "\n",
        "\n",
        "def light_color(color: SpaceDebugColor):\n",
        "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
        "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
        "    return color\n",
        "\n",
        "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
        "    def __init__(self, surface: pygame.Surface) -> None:\n",
        "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
        "\n",
        "        Typical usage::\n",
        "\n",
        "        >>> import pymunk\n",
        "        >>> surface = pygame.Surface((10,10))\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        You can control the color of a shape by setting shape.color to the color\n",
        "        you want it drawn in::\n",
        "\n",
        "        >>> c = pymunk.Circle(None, 10)\n",
        "        >>> c.color = pygame.Color(\"pink\")\n",
        "\n",
        "        See pygame_util.demo.py for a full example\n",
        "\n",
        "        Since pygame uses a coordiante system where y points down (in contrast\n",
        "        to many other cases), you either have to make the physics simulation\n",
        "        with Pymunk also behave in that way, or flip everything when you draw.\n",
        "\n",
        "        The easiest is probably to just make the simulation behave the same\n",
        "        way as Pygame does. In that way all coordinates used are in the same\n",
        "        orientation and easy to reason about::\n",
        "\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> space.gravity = (0, -1000)\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        To flip the drawing its possible to set the module property\n",
        "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
        "        the simulation upside down before drawing::\n",
        "\n",
        "        >>> positive_y_is_up = True\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0)\n",
        "        >>> # Body will be position in bottom left corner\n",
        "\n",
        "        :Parameters:\n",
        "                surface : pygame.Surface\n",
        "                    Surface that the objects will be drawn on\n",
        "        \"\"\"\n",
        "        self.surface = surface\n",
        "        super(DrawOptions, self).__init__()\n",
        "\n",
        "    def draw_circle(\n",
        "        self,\n",
        "        pos: Vec2d,\n",
        "        angle: float,\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "\n",
        "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
        "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
        "\n",
        "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
        "        p2 = to_pygame(circle_edge, self.surface)\n",
        "        line_r = 2 if radius > 20 else 1\n",
        "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
        "\n",
        "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
        "\n",
        "    def draw_fat_segment(\n",
        "        self,\n",
        "        a: Tuple[float, float],\n",
        "        b: Tuple[float, float],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        r = round(max(1, radius * 2))\n",
        "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
        "        if r > 2:\n",
        "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
        "            if orthog[0] == 0 and orthog[1] == 0:\n",
        "                return\n",
        "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
        "            orthog[0] = round(orthog[0] * scale)\n",
        "            orthog[1] = round(orthog[1] * scale)\n",
        "            points = [\n",
        "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
        "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
        "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
        "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
        "            ]\n",
        "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p1[0]), round(p1[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p2[0]), round(p2[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "\n",
        "    def draw_polygon(\n",
        "        self,\n",
        "        verts: Sequence[Tuple[float, float]],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        ps = [to_pygame(v, self.surface) for v in verts]\n",
        "        ps += [ps[0]]\n",
        "\n",
        "        radius = 2\n",
        "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
        "\n",
        "        if radius > 0:\n",
        "            for i in range(len(verts)):\n",
        "                a = verts[i]\n",
        "                b = verts[(i + 1) % len(verts)]\n",
        "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
        "\n",
        "    def draw_dot(\n",
        "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
        "\n",
        "\n",
        "def pymunk_to_shapely(body, shapes):\n",
        "    geoms = list()\n",
        "    for shape in shapes:\n",
        "        if isinstance(shape, pymunk.shapes.Poly):\n",
        "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
        "            verts += [verts[0]]\n",
        "            geoms.append(sg.Polygon(verts))\n",
        "        else:\n",
        "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
        "    geom = sg.MultiPolygon(geoms)\n",
        "    return geom\n",
        "\n",
        "# env\n",
        "class PushTEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "    reward_range = (0., 1.)\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False,\n",
        "            block_cog=None, damping=None,\n",
        "            render_action=True,\n",
        "            render_size=96,\n",
        "            reset_to_state=None\n",
        "        ):\n",
        "        self._seed = None\n",
        "        self.seed()\n",
        "        self.window_size = ws = 512  # The size of the PyGame window\n",
        "        self.render_size = render_size\n",
        "        self.sim_hz = 100\n",
        "        # Local controller params.\n",
        "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
        "        self.control_hz = self.metadata['video.frames_per_second']\n",
        "        # legcay set_state for data compatiblity\n",
        "        self.legacy = legacy\n",
        "\n",
        "        # agent_pos, block_pos, block_angle\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
        "            shape=(5,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # positional goal for agent\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws], dtype=np.float64),\n",
        "            shape=(2,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        self.block_cog = block_cog\n",
        "        self.damping = damping\n",
        "        self.render_action = render_action\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.screen = None\n",
        "\n",
        "        self.space = None\n",
        "        self.teleop = None\n",
        "        self.render_buffer = None\n",
        "        self.latest_action = None\n",
        "        self.reset_to_state = reset_to_state\n",
        "\n",
        "    def reset(self):\n",
        "        seed = self._seed\n",
        "        self._setup()\n",
        "        if self.block_cog is not None:\n",
        "            self.block.center_of_gravity = self.block_cog\n",
        "        if self.damping is not None:\n",
        "            self.space.damping = self.damping\n",
        "\n",
        "        # use legacy RandomState for compatiblity\n",
        "        state = self.reset_to_state\n",
        "        if state is None:\n",
        "            rs = np.random.RandomState(seed=seed)\n",
        "            state = np.array([\n",
        "                rs.randint(50, 450), rs.randint(50, 450),\n",
        "                rs.randint(100, 400), rs.randint(100, 400),\n",
        "                rs.randn() * 2 * np.pi - np.pi\n",
        "                ])\n",
        "        self._set_state(state)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = self._get_info()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        dt = 1.0 / self.sim_hz\n",
        "        self.n_contact_points = 0\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        if action is not None:\n",
        "            self.latest_action = action\n",
        "            for i in range(n_steps):\n",
        "                # Step PD control.\n",
        "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
        "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
        "                self.agent.velocity += acceleration * dt\n",
        "\n",
        "                # Step physics.\n",
        "                self.space.step(dt)\n",
        "\n",
        "        # compute reward\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
        "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
        "\n",
        "        intersection_area = goal_geom.intersection(block_geom).area\n",
        "        goal_area = goal_geom.area\n",
        "        coverage = intersection_area / goal_area\n",
        "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
        "        done = coverage > self.success_threshold\n",
        "        terminated = done\n",
        "        truncated = done\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode):\n",
        "        return self._render_frame(mode)\n",
        "\n",
        "    def teleop_agent(self):\n",
        "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
        "        def act(obs):\n",
        "            act = None\n",
        "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
        "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
        "                self.teleop = True\n",
        "                act = mouse_position\n",
        "            return act\n",
        "        return TeleopAgent(act)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array(\n",
        "            tuple(self.agent.position) \\\n",
        "            + tuple(self.block.position) \\\n",
        "            + (self.block.angle % (2 * np.pi),))\n",
        "        return obs\n",
        "\n",
        "    def _get_goal_pose_body(self, pose):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        # preserving the legacy assignment order for compatibility\n",
        "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
        "        body.position = pose[:2].tolist()\n",
        "        body.angle = pose[2]\n",
        "        return body\n",
        "\n",
        "    def _get_info(self):\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
        "        info = {\n",
        "            'pos_agent': np.array(self.agent.position),\n",
        "            'vel_agent': np.array(self.agent.velocity),\n",
        "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
        "            'goal_pose': self.goal_pose,\n",
        "            'n_contacts': n_contact_points_per_step}\n",
        "        return info\n",
        "\n",
        "    def _render_frame(self, mode):\n",
        "\n",
        "        if self.window is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        self.screen = canvas\n",
        "\n",
        "        draw_options = DrawOptions(canvas)\n",
        "\n",
        "        # Draw goal pose.\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        for shape in self.block.shapes:\n",
        "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
        "            goal_points += [goal_points[0]]\n",
        "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
        "\n",
        "        # Draw agent and block.\n",
        "        self.space.debug_draw(draw_options)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # the clock is aleady ticked during in step for \"human\"\n",
        "\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
        "        if self.render_action:\n",
        "            if self.render_action and (self.latest_action is not None):\n",
        "                action = np.array(self.latest_action)\n",
        "                coord = (action / 512 * 96).astype(np.int32)\n",
        "                marker_size = int(8/96*self.render_size)\n",
        "                thickness = int(1/96*self.render_size)\n",
        "                cv2.drawMarker(img, coord,\n",
        "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                    markerSize=marker_size, thickness=thickness)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        if seed is None:\n",
        "            seed = np.random.randint(0,25536)\n",
        "        self._seed = seed\n",
        "        self.np_random = np.random.default_rng(seed)\n",
        "\n",
        "    def _handle_collision(self, arbiter, space, data):\n",
        "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
        "\n",
        "    def _set_state(self, state):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = state.tolist()\n",
        "        pos_agent = state[:2]\n",
        "        pos_block = state[2:4]\n",
        "        rot_block = state[4]\n",
        "        self.agent.position = pos_agent\n",
        "        # setting angle rotates with respect to center of mass\n",
        "        # therefore will modify the geometric position\n",
        "        # if not the same as CoM\n",
        "        # therefore should be modified first.\n",
        "        if self.legacy:\n",
        "            # for compatiblity with legacy data\n",
        "            self.block.position = pos_block\n",
        "            self.block.angle = rot_block\n",
        "        else:\n",
        "            self.block.angle = rot_block\n",
        "            self.block.position = pos_block\n",
        "\n",
        "        # Run physics to take effect\n",
        "        self.space.step(1.0 / self.sim_hz)\n",
        "\n",
        "    def _set_state_local(self, state_local):\n",
        "        agent_pos_local = state_local[:2]\n",
        "        block_pose_local = state_local[2:]\n",
        "        tf_img_obj = st.AffineTransform(\n",
        "            translation=self.goal_pose[:2],\n",
        "            rotation=self.goal_pose[2])\n",
        "        tf_obj_new = st.AffineTransform(\n",
        "            translation=block_pose_local[:2],\n",
        "            rotation=block_pose_local[2]\n",
        "        )\n",
        "        tf_img_new = st.AffineTransform(\n",
        "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
        "        )\n",
        "        agent_pos_new = tf_img_new(agent_pos_local)\n",
        "        new_state = np.array(\n",
        "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
        "                + [tf_img_new.rotation])\n",
        "        self._set_state(new_state)\n",
        "        return new_state\n",
        "\n",
        "    def _setup(self):\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = 0, 0\n",
        "        self.space.damping = 0\n",
        "        self.teleop = False\n",
        "        self.render_buffer = list()\n",
        "\n",
        "        # Add walls.\n",
        "        walls = [\n",
        "            self._add_segment((5, 506), (5, 5), 2),\n",
        "            self._add_segment((5, 5), (506, 5), 2),\n",
        "            self._add_segment((506, 5), (506, 506), 2),\n",
        "            self._add_segment((5, 506), (506, 506), 2)\n",
        "        ]\n",
        "        self.space.add(*walls)\n",
        "\n",
        "        # Add agent, block, and goal zone.\n",
        "        self.agent = self.add_circle((256, 400), 15)\n",
        "        self.block = self.add_tee((256, 300), 0)\n",
        "        self.goal_color = pygame.Color('LightGreen')\n",
        "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
        "\n",
        "        # Add collision handeling\n",
        "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
        "        self.collision_handeler.post_solve = self._handle_collision\n",
        "        self.n_contact_points = 0\n",
        "\n",
        "        self.max_score = 50 * 100\n",
        "        self.success_threshold = 0.95    # 95% coverage.\n",
        "\n",
        "    def _add_segment(self, a, b, radius):\n",
        "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
        "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
        "        return shape\n",
        "\n",
        "    def add_circle(self, position, radius):\n",
        "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
        "        body.position = position\n",
        "        body.friction = 1\n",
        "        shape = pymunk.Circle(body, radius)\n",
        "        shape.color = pygame.Color('RoyalBlue')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_box(self, position, height, width):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        body.position = position\n",
        "        shape = pymunk.Poly.create_box(body, (height, width))\n",
        "        shape.color = pygame.Color('LightSlateGray')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
        "        mass = 1\n",
        "        length = 4\n",
        "        vertices1 = [(-length*scale/2, scale),\n",
        "                                 ( length*scale/2, scale),\n",
        "                                 ( length*scale/2, 0),\n",
        "                                 (-length*scale/2, 0)]\n",
        "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        vertices2 = [(-scale/2, scale),\n",
        "                                 (-scale/2, length*scale),\n",
        "                                 ( scale/2, length*scale),\n",
        "                                 ( scale/2, scale)]\n",
        "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
        "        shape1 = pymunk.Poly(body, vertices1)\n",
        "        shape2 = pymunk.Poly(body, vertices2)\n",
        "        shape1.color = pygame.Color(color)\n",
        "        shape2.color = pygame.Color(color)\n",
        "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
        "        body.position = position\n",
        "        body.angle = angle\n",
        "        body.friction = 1\n",
        "        self.space.add(body, shape1, shape2)\n",
        "        return body\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OknH8Qfqrtc9",
        "outputId": "7563449e-9549-4de2-e049-bde843e76fed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sfchen/miniforge3/envs/robodiff/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argv[0]=\n",
            "Obs:\n",
            "\tblock_translation: (0.43071725041927583, -0.3154978396411959)\n",
            "\tblock_orientation: [1.1064]\n",
            "\tblock2_translation: (0.31391576244968816, -0.08439863792609263)\n",
            "\tblock2_orientation: [1.1014]\n",
            "\teffector_translation: [ 0.3268 -0.487 ]\n",
            "\teffector_target_translation: [ 0.329  -0.4977]\n",
            "\ttarget_translation: [0.2774 0.2024]\n",
            "\ttarget_orientation: [3.1367]\n",
            "\ttarget2_translation: [0.5232 0.1961]\n",
            "\ttarget2_orientation: [0.0148]\n",
            "Action:  array([ 0.029 , -0.0977], dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "from diffusion_policy.env.block_pushing.block_pushing import BlockPush, BlockPushNormalized\n",
        "from diffusion_policy.env.block_pushing.block_pushing_discontinuous import BlockPushDiscontinuous\n",
        "from diffusion_policy.env.block_pushing.block_pushing_multimodal import BlockPushMultimodal\n",
        "#@markdown ### **Env Demo**\n",
        "#@markdown Standard Gym Env (0.21.0 API)\n",
        "\n",
        "# 0. create env object\n",
        "# env = PushTEnv()\n",
        "# env = BlockPush()\n",
        "# env = BlockPushNormalized()\n",
        "# env = BlockPushDiscontinuous()\n",
        "env = BlockPushMultimodal()\n",
        "\n",
        "# 1. seed env for initial state.\n",
        "# Seed 0-200 are used for the demonstration dataset.\n",
        "env.seed(1000)\n",
        "\n",
        "# 2. must reset before use\n",
        "# obs, DEFAULT_IGNORE_PATTERNS = env.reset()\n",
        "obs = env.reset()\n",
        "\n",
        "# 3. 2D positional action space [0,512]\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Standard gym step method\n",
        "# obs, reward, terminated, truncated, info = env.step(action)\n",
        "obs, reward, terminated, info = env.step(action)\n",
        "\n",
        "# prints and explains each dimension of the observation and action vectors\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "    # print(\"Obs: \", repr(obs))\n",
        "    # print(\"Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\")\n",
        "    print('Obs:')\n",
        "    for k,v in obs.items():\n",
        "        print(f\"\\t{k}: {v}\")\n",
        "    print(\"Action: \", repr(action))\n",
        "    # print(\"Action:   [target_agent_x, target_agent_y]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "vHepJOFBucwg"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Dataset**\n",
        "#@markdown\n",
        "#@markdown Defines `PushTStateDataset` and helper functions\n",
        "#@markdown\n",
        "#@markdown The dataset class\n",
        "#@markdown - Load data (obs, action) from a zarr storage\n",
        "#@markdown - Normalizes each dimension of obs and action to [-1,1]\n",
        "#@markdown - Returns\n",
        "#@markdown  - All possible segments with length `pred_horizon`\n",
        "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
        "#@markdown  - key `obs`: shape (obs_horizon, obs_dim)\n",
        "#@markdown  - key `action`: shape (pred_horizon, action_dim)\n",
        "\n",
        "def create_sample_indices(\n",
        "        episode_ends:np.ndarray, sequence_length:int,\n",
        "        pad_before: int=0, pad_after: int=0):\n",
        "    indices = list()\n",
        "    for i in range(len(episode_ends)):\n",
        "        start_idx = 0\n",
        "        if i > 0:\n",
        "            start_idx = episode_ends[i-1]\n",
        "        end_idx = episode_ends[i]\n",
        "        episode_length = end_idx - start_idx\n",
        "\n",
        "        min_start = -pad_before\n",
        "        max_start = episode_length - sequence_length + pad_after\n",
        "\n",
        "        # range stops one idx before end\n",
        "        for idx in range(min_start, max_start+1):\n",
        "            buffer_start_idx = max(idx, 0) + start_idx\n",
        "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
        "            start_offset = buffer_start_idx - (idx+start_idx)\n",
        "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
        "            sample_start_idx = 0 + start_offset\n",
        "            sample_end_idx = sequence_length - end_offset\n",
        "            indices.append([\n",
        "                buffer_start_idx, buffer_end_idx,\n",
        "                sample_start_idx, sample_end_idx])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length,\n",
        "                    buffer_start_idx, buffer_end_idx,\n",
        "                    sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(\n",
        "                shape=(sequence_length,) + input_arr.shape[1:],\n",
        "                dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "# normalize data\n",
        "def get_data_stats(data):\n",
        "    data = data.reshape(-1,data.shape[-1])\n",
        "    stats = {\n",
        "        'min': np.min(data, axis=0),\n",
        "        'max': np.max(data, axis=0)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def normalize_data(data, stats):\n",
        "    # nomalize to [0,1]\n",
        "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
        "    # normalize to [-1, 1]\n",
        "    ndata = ndata * 2 - 1\n",
        "    return ndata\n",
        "\n",
        "def unnormalize_data(ndata, stats):\n",
        "    ndata = (ndata + 1) / 2\n",
        "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
        "    return data\n",
        "\n",
        "# dataset\n",
        "class PushTStateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_path,\n",
        "                 pred_horizon, obs_horizon, action_horizon):\n",
        "\n",
        "        # read from zarr dataset\n",
        "        dataset_root = zarr.open(dataset_path, 'r')\n",
        "        # All demonstration episodes are concatinated in the first dimension N\n",
        "        train_data = {\n",
        "            # (N, action_dim)\n",
        "            'action': dataset_root['data']['action'][:],\n",
        "            # (N, obs_dim)\n",
        "            'obs': dataset_root['data']['state'][:]\n",
        "        }\n",
        "        # Marks one-past the last index for each episode\n",
        "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
        "\n",
        "        # compute start and end of each state-action sequence\n",
        "        # also handles padding\n",
        "        indices = create_sample_indices(\n",
        "            episode_ends=episode_ends,\n",
        "            sequence_length=pred_horizon,\n",
        "            # add padding such that each timestep in the dataset are seen\n",
        "            pad_before=obs_horizon-1,\n",
        "            pad_after=action_horizon-1)\n",
        "\n",
        "        # compute statistics and normalized data to [-1,1]\n",
        "        stats = dict()\n",
        "        normalized_train_data = dict()\n",
        "        for key, data in train_data.items():\n",
        "            stats[key] = get_data_stats(data)\n",
        "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
        "\n",
        "        self.indices = indices\n",
        "        self.stats = stats\n",
        "        self.normalized_train_data = normalized_train_data\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "\n",
        "    def __len__(self):\n",
        "        # all possible segments of the dataset\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get the start/end indices for this datapoint\n",
        "        buffer_start_idx, buffer_end_idx, \\\n",
        "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        # get nomralized data using these indices\n",
        "        nsample = sample_sequence(\n",
        "            train_data=self.normalized_train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations\n",
        "        nsample['obs'] = nsample['obs'][:self.obs_horizon,:]\n",
        "        return nsample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZiHF3lzvB6k",
        "outputId": "474315be-970f-4a47-ca7e-d27d92a1711f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['obs'].shape: torch.Size([256, 16, 16])\n",
            "batch['action'].shape torch.Size([256, 16, 2])\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Dataset Demo**\n",
        "from diffusion_policy.dataset.blockpush_lowdim_dataset import BlockPushLowdimDataset\n",
        "\n",
        "# download demonstration data from Google Drive\n",
        "# dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
        "dataset_path = \"data/training/block_pushing/multimodal_push_seed_abs.zarr\"\n",
        "# if not os.path.isfile(dataset_path):\n",
        "    # id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
        "    # gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "\n",
        "# parameters\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "#|o|o|                             observations: 2\n",
        "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
        "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
        "\n",
        "# create dataset from file\n",
        "# dataset = PushTStateDataset(\n",
        "    # dataset_path=dataset_path,\n",
        "    # pred_horizon=pred_horizon,\n",
        "    # obs_horizon=obs_horizon,\n",
        "    # action_horizon=action_horizon\n",
        "# )\n",
        "dataset = BlockPushLowdimDataset(\n",
        "    zarr_path=dataset_path,\n",
        "    horizon=pred_horizon,\n",
        "    pad_before=obs_horizon-1,\n",
        "    pad_after=action_horizon-1\n",
        ")\n",
        "# save training data statistics (min, max) for each dim\n",
        "# stats = dataset.stats\n",
        "\n",
        "# create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    num_workers=1,\n",
        "    shuffle=True,\n",
        "    # accelerate cpu-gpu transfer\n",
        "    pin_memory=True,\n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(dataloader))\n",
        "print(\"batch['obs'].shape:\", batch['obs'].shape)\n",
        "print(\"batch['action'].shape\", batch['action'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sfchen/miniforge3/envs/robodiff/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argv[0]=\n",
            "Trajectory visualization saved as block_push_trajectory.mp4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from diffusion_policy.env.block_pushing.block_pushing_multimodal import BlockPushMultimodal\n",
        "from diffusion_policy.dataset.blockpush_lowdim_dataset import BlockPushLowdimDataset\n",
        "from matplotlib.patches import Rectangle, Circle\n",
        "\n",
        "# Parameters\n",
        "dataset_path = \"data/training/block_pushing/multimodal_push_seed_abs.zarr\"\n",
        "pred_horizon = 128\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "\n",
        "# Initialize Dataset and Dataloader\n",
        "dataset = BlockPushLowdimDataset(\n",
        "    zarr_path=dataset_path,\n",
        "    horizon=pred_horizon,\n",
        "    pad_before=obs_horizon - 1,\n",
        "    pad_after=action_horizon - 1\n",
        ")\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,  # Use batch_size=1 for visualization purposes\n",
        "    num_workers=1,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# Load a batch from the dataset\n",
        "for i in range(50):\n",
        "    batch = next(iter(dataloader))\n",
        "    obs_batch = batch['obs']  # Shape: [batch_size, time, dim]\n",
        "    action_batch = batch['action']  # Shape: [batch_size, time, dim]\n",
        "\n",
        "# Create environment\n",
        "env = BlockPushMultimodal()\n",
        "\n",
        "print(\"Trajectory visualization saved as block_push_trajectory.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered dataset1 created containing trajectories involving pushing block1 to region1 or region2 and excluding cases where two blocks move together.\n",
            "Filtered dataset2 created containing trajectories involving pushing block1 or block2 to region1.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from diffusion_policy.env.block_pushing.block_pushing_multimodal import BlockPushMultimodal\n",
        "from diffusion_policy.dataset.blockpush_lowdim_dataset import BlockPushLowdimDataset\n",
        "from matplotlib.patches import Rectangle, Circle\n",
        "from tqdm import tqdm\n",
        "from torch.optim.swa_utils import AveragedModel as EMAModel\n",
        "\n",
        "# Parameters\n",
        "dataset_path = \"data/training/block_pushing/multimodal_push_seed_abs.zarr\"\n",
        "pred_horizon = 128\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "\n",
        "# Initialize Dataset and Dataloader\n",
        "dataset = BlockPushLowdimDataset(\n",
        "    zarr_path=dataset_path,\n",
        "    horizon=pred_horizon,\n",
        "    pad_before=obs_horizon - 1,\n",
        "    pad_after=action_horizon - 1\n",
        ")\n",
        "\n",
        "# Function to split trajectories into two parts\n",
        "class SplitBlockPushLowdimDataset(BlockPushLowdimDataset):\n",
        "    def __init__(self, original_dataset, dataset_path, split=\"first\"):\n",
        "        super().__init__(\n",
        "            zarr_path=dataset_path,\n",
        "            horizon=original_dataset.horizon,\n",
        "            pad_before=original_dataset.pad_before,\n",
        "            pad_after=original_dataset.pad_after,\n",
        "            obs_key=original_dataset.obs_key,\n",
        "            action_key=original_dataset.action_key,\n",
        "            obs_eef_target=original_dataset.obs_eef_target,\n",
        "            use_manual_normalizer=original_dataset.use_manual_normalizer,\n",
        "            seed=42,\n",
        "            val_ratio=0.0\n",
        "        )\n",
        "        self.split = split\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        sample = self.sampler.sample_sequence(idx)\n",
        "        data = self._sample_to_data(sample)\n",
        "\n",
        "        # Split the trajectory into two halves\n",
        "        split_index = data['obs'].shape[0] // 2\n",
        "        if self.split == \"first\":\n",
        "            obs = data['obs'][:split_index]\n",
        "            action = data['action'][:split_index]\n",
        "        elif self.split == \"second\":\n",
        "            obs = data['obs'][split_index:]\n",
        "            action = data['action'][split_index:]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split: {self.split}\")\n",
        "\n",
        "        torch_data = {\n",
        "            'obs': torch.from_numpy(obs),\n",
        "            'action': torch.from_numpy(action)\n",
        "        }\n",
        "        return torch_data\n",
        "\n",
        "# Create split datasets\n",
        "dataset_first_half = SplitBlockPushLowdimDataset(dataset, dataset_path=dataset_path, split=\"first\")\n",
        "dataset_second_half = SplitBlockPushLowdimDataset(dataset, dataset_path=dataset_path, split=\"second\")\n",
        "\n",
        "# Function to filter out trajectories where two blocks move together\n",
        "# This filter will exclude any trajectory where the blocks are seen moving simultaneously (i.e., interacting in a way that suggests they are both affected by the effector)\n",
        "def filter_no_two_blocks_moving_together(data):\n",
        "    obs = data['obs']\n",
        "    threshold_distance = 0.05  # Minimum distance to consider as \"too close\"\n",
        "    close_count = 0  # Counter for how many consecutive timesteps the blocks are close\n",
        "\n",
        "    for t in range(1, len(obs)):\n",
        "        block1_translation = obs[t][:2]\n",
        "        block2_translation = obs[t][3:5]\n",
        "        distance_between_blocks = np.linalg.norm(block1_translation - block2_translation)\n",
        "\n",
        "        # Check if the blocks are close together\n",
        "        if distance_between_blocks < threshold_distance:\n",
        "            close_count += 1\n",
        "            # If blocks are close for at least 10 consecutive timesteps, filter out this trajectory\n",
        "            if close_count >= 10:\n",
        "                return False\n",
        "        else:\n",
        "            close_count = 0\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# Function to filter dataset to create dataset1 (pushing block1 to region1 or region2)\n",
        "def filter_block1_to_region1_or_region2(data):\n",
        "    obs = data['obs']\n",
        "    block2_translation_initial = obs[0][3:5]\n",
        "    block2_translation_final = obs[-1][3:5]\n",
        "    block2_orientation_initial = obs[0][5]\n",
        "    block2_orientation_final = obs[-1][5]\n",
        "    # Ensure block2 does not change its position or orientation throughout timesteps\n",
        "    if (not np.allclose(block2_translation_initial, block2_translation_final, atol=1e-3) or\n",
        "            not np.isclose(block2_orientation_initial, block2_orientation_final, atol=1e-3)):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Function to filter dataset\n",
        "class FilteredBlockPushLowdimDataset(BlockPushLowdimDataset):\n",
        "    def __init__(self, original_dataset, filter_fn):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.filter_fn = filter_fn\n",
        "        self.filtered_indices = [i for i in range(len(original_dataset)) if filter_fn(original_dataset[i])]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.original_dataset[self.filtered_indices[idx]]\n",
        "\n",
        "# Create filtered dataset1 using both first and second half datasets\n",
        "filtered_dataset1_first_half = FilteredBlockPushLowdimDataset(dataset_first_half, filter_block1_to_region1_or_region2)\n",
        "filtered_dataset1_second_half = FilteredBlockPushLowdimDataset(dataset_second_half, filter_block1_to_region1_or_region2)\n",
        "\n",
        "# Apply filter to remove trajectories where two blocks move together for dataset1\n",
        "filtered_dataset1_first_half = FilteredBlockPushLowdimDataset(filtered_dataset1_first_half, filter_no_two_blocks_moving_together)\n",
        "filtered_dataset1_second_half = FilteredBlockPushLowdimDataset(filtered_dataset1_second_half, filter_no_two_blocks_moving_together)\n",
        "\n",
        "# Combine filtered datasets to create dataset1\n",
        "dataset1 = torch.utils.data.ConcatDataset([filtered_dataset1_first_half, filtered_dataset1_second_half])\n",
        "\n",
        "print(\"Filtered dataset1 created containing trajectories involving pushing block1 to region1 or region2 and excluding cases where two blocks move together.\")\n",
        "\n",
        "# Function to filter dataset to create dataset2 (pushing block1 or block2 to region1)\n",
        "def filter_block1_or_block2_to_region1(data):\n",
        "    obs = data['obs']\n",
        "    # Extract region centers directly from the observation data\n",
        "    region1_center = obs[0][10:12]  # Target translation (region 1)\n",
        "    region2_center = obs[0][13:15]  # Target2 translation (region 2)\n",
        "    region_radius = 0.05  # Radius to define the vicinity of the region\n",
        "\n",
        "    # Check if no block is near region 1 at timestep 1\n",
        "    block1_translation_initial = obs[0][:2]\n",
        "    block2_translation_initial = obs[0][3:5]\n",
        "    if (np.linalg.norm(block1_translation_initial - region1_center) < region_radius or\n",
        "            np.linalg.norm(block2_translation_initial - region1_center) < region_radius):\n",
        "        return False\n",
        "\n",
        "    # Check if either block1 or block2 is near region 1 at the last timestep\n",
        "    block1_translation_final = obs[-1][:2]\n",
        "    block2_translation_final = obs[-1][3:5]\n",
        "    if (np.linalg.norm(block1_translation_final - region1_center) > region_radius and\n",
        "            np.linalg.norm(block2_translation_final - region1_center) > region_radius):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Create filtered dataset2 using both first and second half datasets\n",
        "filtered_dataset2_first_half = FilteredBlockPushLowdimDataset(dataset_first_half, filter_block1_or_block2_to_region1)\n",
        "filtered_dataset2_second_half = FilteredBlockPushLowdimDataset(dataset_second_half, filter_block1_or_block2_to_region1)\n",
        "\n",
        "# Apply filter to remove trajectories where two blocks move together for dataset2\n",
        "filtered_dataset2_first_half = FilteredBlockPushLowdimDataset(filtered_dataset2_first_half, filter_no_two_blocks_moving_together)\n",
        "filtered_dataset2_second_half = FilteredBlockPushLowdimDataset(filtered_dataset2_second_half, filter_no_two_blocks_moving_together)\n",
        "\n",
        "# Combine filtered datasets to create dataset2\n",
        "dataset2 = torch.utils.data.ConcatDataset([filtered_dataset2_first_half, filtered_dataset2_second_half])\n",
        "print(\"Filtered dataset2 created containing trajectories involving pushing block1 or block2 to region1.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization function for dataset1 and dataset2\n",
        "def visualize_trajectory(obs, actions, save_path=\"trajectory.mp4\"):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-0.5, 1.5)\n",
        "    ax.set_ylim(-0.5, 1.5)\n",
        "    agent_patch = Rectangle((0, 0), 0.02, 0.02, fc='blue', label='Effector')\n",
        "    block_patch = Rectangle((0, 0), 0.03, 0.03, fc='red', label='Block')\n",
        "    block2_patch = Rectangle((0, 0), 0.03, 0.03, fc='green', label='Block 2')\n",
        "    target_patch = Circle((0, 0), 0.02, fc='orange', label='Target')\n",
        "    target2_patch = Circle((0, 0), 0.02, fc='purple', label='Target 2')\n",
        "    region1_circle = Circle((0, 0), 0.05, fill=False, edgecolor='orange', linestyle='--', label='Region 1 Boundary')\n",
        "    region2_circle = Circle((0, 0), 0.05, fill=False, edgecolor='purple', linestyle='--', label='Region 2 Boundary')\n",
        "    ax.add_patch(agent_patch)\n",
        "    ax.add_patch(block_patch)\n",
        "    ax.add_patch(block2_patch)\n",
        "    ax.add_patch(target_patch)\n",
        "    ax.add_patch(target2_patch)\n",
        "    ax.add_patch(region1_circle)\n",
        "    ax.add_patch(region2_circle)\n",
        "    plt.legend()\n",
        "\n",
        "    def update(frame):\n",
        "        block_translation = obs[frame][:2]\n",
        "        block_orientation = obs[frame][2]\n",
        "        block2_translation = obs[frame][3:5]\n",
        "        block2_orientation = obs[frame][5]\n",
        "        effector_translation = obs[frame][6:8]\n",
        "        effector_target_translation = obs[frame][8:10]\n",
        "        target_translation = obs[frame][10:12]\n",
        "        target_orientation = obs[frame][12]\n",
        "        target2_translation = obs[frame][13:15]\n",
        "        target2_orientation = obs[frame][15]\n",
        "        target_agent_x, target_agent_y = actions[frame][:2]\n",
        "        agent_patch.set_xy((effector_translation[0], effector_translation[1]))\n",
        "        block_patch.set_xy((block_translation[0], block_translation[1]))\n",
        "        block2_patch.set_xy((block2_translation[0], block2_translation[1]))\n",
        "        target_patch.set_center((target_translation[0], target_translation[1]))\n",
        "        target2_patch.set_center((target2_translation[0], target2_translation[1]))\n",
        "        region1_circle.set_center((target_translation[0], target_translation[1]))\n",
        "        region2_circle.set_center((target2_translation[0], target2_translation[1]))\n",
        "        ax.set_title(f\"Frame: {frame}, Target Agent: ({target_agent_x:.2f}, {target_agent_y:.2f}\" + \"\\n\"\n",
        "                     f\"Frame: {frame}, Current Agent: ({effector_translation[0]:.2f}, {effector_target_translation[1]:.2f})\")\n",
        "        return agent_patch, block_patch, block2_patch, target_patch, target2_patch, region1_circle, region2_circle\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(obs), blit=False, repeat=False)\n",
        "    ani.save(save_path, writer='ffmpeg', fps=5)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:07,494 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:07,496 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset1/dataset1_trajectory_640.mp4\n",
            "2024-11-26 19:11:20,199 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:20,201 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset1/dataset1_trajectory_688.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset1 trajectory 640 visualization saved as visualizations/dataset1/dataset1_trajectory_640.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:34,781 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:34,783 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset1/dataset1_trajectory_491.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset1 trajectory 688 visualization saved as visualizations/dataset1/dataset1_trajectory_688.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:46,969 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:46,971 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset2/dataset2_trajectory_34.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset1 trajectory 491 visualization saved as visualizations/dataset1/dataset1_trajectory_491.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:57,356 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:57,358 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset2/dataset2_trajectory_270.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset2 trajectory 34 visualization saved as visualizations/dataset2/dataset2_trajectory_34.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:12:07,682 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:12:07,684 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset2/dataset2_trajectory_1304.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset2 trajectory 270 visualization saved as visualizations/dataset2/dataset2_trajectory_270.mp4\n",
            "dataset2 trajectory 1304 visualization saved as visualizations/dataset2/dataset2_trajectory_1304.mp4\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def visualize_dataset(dataset, save_dir, dataset_name):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    indices = random.sample(range(len(dataset)), min(len(dataset), 3))  # Pick 10 random trajectories for visualization\n",
        "    for i in indices:  # Visualize up to 5 trajectories for verification\n",
        "        data = dataset[i]\n",
        "        obs = data['obs'].numpy()\n",
        "        actions = data['action'].numpy()\n",
        "        save_path = os.path.join(save_dir, f\"{dataset_name}_trajectory_{i}.mp4\")\n",
        "        visualize_trajectory(obs, actions, save_path)\n",
        "        print(f\"{dataset_name} trajectory {i} visualization saved as {save_path}\")\n",
        "\n",
        "# Visualize and save trajectories from dataset1 and dataset2\n",
        "visualize_dataset(dataset1, save_dir=\"visualizations/dataset1\", dataset_name=\"dataset1\")\n",
        "visualize_dataset(dataset2, save_dir=\"visualizations/dataset2\", dataset_name=\"dataset2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-27 11:23:42,037 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:23:42,039 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset1/dataset1_trajectory_0_block1_region2.mp4\n",
            "2024-11-27 11:23:57,921 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:23:57,924 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset1/dataset1_trajectory_7_block1_region1.mp4\n",
            "2024-11-27 11:24:12,710 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:24:12,713 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset2/dataset2_trajectory_0_block1_region1.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory categorization statistics:\n",
            "pushing_block1_to_region1: 621\n",
            "pushing_block2_to_region1: 0\n",
            "pushing_block1_to_region2: 674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-27 11:24:26,192 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:24:26,193 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset2_trajectory_2_block2_region1.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory categorization statistics:\n",
            "pushing_block1_to_region1: 723\n",
            "pushing_block2_to_region1: 845\n",
            "pushing_block1_to_region2: 0\n"
          ]
        }
      ],
      "source": [
        "def categorize_trajectories(dataset, dataset_name):\n",
        "    categories = {\n",
        "        \"pushing_block1_to_region1\": 0,\n",
        "        \"pushing_block2_to_region1\": 0,\n",
        "        \"pushing_block1_to_region2\": 0\n",
        "    }\n",
        "    pushing_block1_to_region2_indices = []  # To record indices of incorrect cases\n",
        "\n",
        "    vs = [0,0,0]\n",
        "    for idx, data in enumerate(dataset):\n",
        "        obs = data['obs'].numpy()\n",
        "\n",
        "        region1_center = obs[0][10:12]\n",
        "        region2_center = obs[0][13:15]\n",
        "\n",
        "        block1_translation_initial = obs[0][:2]\n",
        "        block2_translation_initial = obs[0][3:5]\n",
        "        block1_translation_final = obs[-1][:2]\n",
        "        block2_translation_final = obs[-1][3:5]\n",
        "        region_radius = 0.05\n",
        "\n",
        "        # Check conditions for pushing block1 to region1\n",
        "        if (np.linalg.norm(block1_translation_initial - region1_center) > region_radius and\n",
        "                np.linalg.norm(block1_translation_final - region1_center) < region_radius):\n",
        "            categories[\"pushing_block1_to_region1\"] += 1\n",
        "\n",
        "            if vs[0] == 0:\n",
        "                actions = data['action'].numpy()\n",
        "                save_path = os.path.join(f'./visualizations/{dataset_name}', f\"{dataset_name}_trajectory_{idx}_block1_region1.mp4\")\n",
        "                visualize_trajectory(obs, actions, save_path)\n",
        "                vs[0] = 1\n",
        "\n",
        "        # Check conditions for pushing block2 to region1\n",
        "        elif (np.linalg.norm(block2_translation_initial - region1_center) > region_radius and\n",
        "                np.linalg.norm(block2_translation_final - region1_center) < region_radius):\n",
        "            categories[\"pushing_block2_to_region1\"] += 1\n",
        "\n",
        "            if vs[1] == 0:\n",
        "                actions = data['action'].numpy()\n",
        "                save_path = os.path.join('./visualizations', f\"{dataset_name}_trajectory_{idx}_block2_region1.mp4\")\n",
        "                visualize_trajectory(obs, actions, save_path)\n",
        "                vs[1] = 1\n",
        "\n",
        "        # Check conditions for pushing block1 to region2\n",
        "        elif (np.linalg.norm(block1_translation_initial - region2_center) > region_radius and\n",
        "                np.linalg.norm(block1_translation_final - region2_center) < region_radius):\n",
        "            categories[\"pushing_block1_to_region2\"] += 1\n",
        "\n",
        "            if vs[2] == 0:\n",
        "                actions = data['action'].numpy()\n",
        "                save_path = os.path.join(f'./visualizations/{dataset_name}', f\"{dataset_name}_trajectory_{idx}_block1_region2.mp4\")\n",
        "                visualize_trajectory(obs, actions, save_path)\n",
        "                vs[2] = 1\n",
        "\n",
        "    print(\"Trajectory categorization statistics:\")\n",
        "    for category, count in categories.items():\n",
        "        print(f\"{category}: {count}\")\n",
        "\n",
        "    # Print the indices of the incorrect cases for dataset2\n",
        "    if pushing_block1_to_region2_indices:\n",
        "        print(\"Indices of trajectories in dataset2 that incorrectly involve pushing block1 to region2:\")\n",
        "        print(pushing_block1_to_region2_indices)\n",
        "        obs = data['obs'].numpy()\n",
        "\n",
        "   \n",
        "\n",
        "# Categorize trajectories in dataset1 and dataset2\n",
        "categorize_trajectories(dataset1, 'dataset1')\n",
        "categorize_trajectories(dataset2, 'dataset2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'filter_block1_to_region1_or_region2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 163\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Create filtered dataset1 using both first and second half datasets\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m filtered_dataset1_first_half \u001b[38;5;241m=\u001b[39m FilteredBlockPushLowdimDataset(dataset_first_half, \u001b[43mfilter_block1_to_region1_or_region2\u001b[49m)\n\u001b[1;32m    164\u001b[0m filtered_dataset1_second_half \u001b[38;5;241m=\u001b[39m FilteredBlockPushLowdimDataset(dataset_second_half, filter_block1_to_region1_or_region2)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Apply filter to remove trajectories where two blocks move together for dataset1\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filter_block1_to_region1_or_region2' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from diffusion_policy.env.block_pushing.block_pushing_multimodal import BlockPushMultimodal\n",
        "from diffusion_policy.dataset.blockpush_lowdim_dataset import BlockPushLowdimDataset\n",
        "from matplotlib.patches import Rectangle, Circle\n",
        "from tqdm import tqdm\n",
        "from torch.optim.swa_utils import AveragedModel as EMAModel\n",
        "from diffusion_policy.common.sampler import SequenceSampler, get_val_mask\n",
        "from typing import Dict\n",
        "\n",
        "# Parameters\n",
        "dataset_path = \"data/training/block_pushing/multimodal_push_seed_abs.zarr\"\n",
        "pred_horizon = 128\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "\n",
        "# Initialize Dataset and Dataloader\n",
        "dataset = BlockPushLowdimDataset(\n",
        "    zarr_path=dataset_path,\n",
        "    horizon=pred_horizon,\n",
        "    pad_before=obs_horizon - 1,\n",
        "    pad_after=action_horizon - 1\n",
        ")\n",
        "\n",
        "# Function to split trajectories into two parts\n",
        "class SplitBlockPushLowdimDataset(BlockPushLowdimDataset):\n",
        "    def __init__(self, original_dataset, split=\"first\"):\n",
        "        # Directly copy the attributes from the original dataset without calling super().__init__()\n",
        "        self.replay_buffer = original_dataset.replay_buffer  # Reuse the original replay buffer\n",
        "        self.obs_key = original_dataset.obs_key\n",
        "        self.action_key = original_dataset.action_key\n",
        "        self.obs_eef_target = original_dataset.obs_eef_target\n",
        "        self.use_manual_normalizer = original_dataset.use_manual_normalizer\n",
        "        self.train_mask = original_dataset.train_mask\n",
        "\n",
        "        # Assign other parameters from original dataset\n",
        "        self.horizon = original_dataset.horizon\n",
        "        self.pad_before = original_dataset.pad_before\n",
        "        self.pad_after = original_dataset.pad_after\n",
        "        self.split = split\n",
        "\n",
        "        # Initialize the sampler to be used for splitting\n",
        "        self.sampler = original_dataset.sampler\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Use the sampler from original dataset to get a sample\n",
        "        sample = self.sampler.sample_sequence(idx)\n",
        "        data = self._sample_to_data(sample)\n",
        "\n",
        "        # Split the trajectory into two halves\n",
        "        split_index = data['obs'].shape[0] // 2\n",
        "        if self.split == \"first\":\n",
        "            obs = data['obs'][:split_index]\n",
        "            action = data['action'][:split_index]\n",
        "        elif self.split == \"second\":\n",
        "            obs = data['obs'][split_index:]\n",
        "            action = data['action'][split_index:]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split: {self.split}\")\n",
        "\n",
        "        # Convert to torch Tensors\n",
        "        torch_data = {\n",
        "            'obs': torch.from_numpy(obs),\n",
        "            'action': torch.from_numpy(action)\n",
        "        }\n",
        "        return torch_data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sampler)\n",
        "\n",
        "\n",
        "# Create split datasets\n",
        "dataset_first_half = SplitBlockPushLowdimDataset(original_dataset=dataset, split=\"first\")\n",
        "dataset_second_half = SplitBlockPushLowdimDataset(original_dataset=dataset, split=\"second\")\n",
        "\n",
        "# Function to filter dataset\n",
        "class FilteredBlockPushLowdimDataset(BlockPushLowdimDataset):\n",
        "    def __init__(self, original_dataset, filter_fn):\n",
        "        # Apply filter to find relevant indices\n",
        "        self.filter_fn = filter_fn\n",
        "        self.filtered_indices = [i for i in range(len(original_dataset)) if filter_fn(original_dataset[i])]\n",
        "\n",
        "        # Assign necessary attributes from the original dataset\n",
        "        self.replay_buffer = original_dataset.replay_buffer  # Inherit replay buffer\n",
        "        self.obs_key = original_dataset.obs_key\n",
        "        self.action_key = original_dataset.action_key\n",
        "        self.obs_eef_target = original_dataset.obs_eef_target\n",
        "        self.use_manual_normalizer = original_dataset.use_manual_normalizer\n",
        "        self.train_mask = original_dataset.train_mask\n",
        "\n",
        "        # Use the filtered indices and reset the sampler accordingly\n",
        "        self.reset_sampler(\n",
        "            horizon=original_dataset.horizon,\n",
        "            pad_before=original_dataset.pad_before,\n",
        "            pad_after=original_dataset.pad_after\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_indices)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Access the filtered index to retrieve the sample\n",
        "        filtered_idx = self.filtered_indices[idx]\n",
        "        sample = self.sampler.sample_sequence(filtered_idx)\n",
        "        data = self._sample_to_data(sample)\n",
        "\n",
        "        # Convert to torch Tensors\n",
        "        torch_data = {\n",
        "            'obs': torch.from_numpy(data['obs']),\n",
        "            'action': torch.from_numpy(data['action'])\n",
        "        }\n",
        "        return torch_data\n",
        "\n",
        "    def reset_sampler(self, horizon, pad_before, pad_after):\n",
        "        \"\"\"\n",
        "        Resets the SequenceSampler with new sequence parameters.\n",
        "        \"\"\"\n",
        "        self.horizon = horizon\n",
        "        self.pad_before = pad_before\n",
        "        self.pad_after = pad_after\n",
        "\n",
        "        # Create a new SequenceSampler with updated sequence parameters\n",
        "        # Use the same replay buffer but filter the episode_mask to only include filtered_indices\n",
        "        filtered_episode_mask = np.zeros(self.replay_buffer.n_episodes, dtype=bool)\n",
        "        filtered_episode_mask[self.filtered_indices] = True\n",
        "\n",
        "        self.sampler = SequenceSampler(\n",
        "            replay_buffer=self.replay_buffer,\n",
        "            sequence_length=horizon,\n",
        "            pad_before=pad_before,\n",
        "            pad_after=pad_after,\n",
        "            episode_mask=filtered_episode_mask\n",
        "        )\n",
        "        print(f\"Sampler reset with horizon={horizon}, pad_before={pad_before}, pad_after={pad_after}\")\n",
        "\n",
        "# Filtering function to remove trajectories where two blocks move together\n",
        "def filter_no_two_blocks_moving_together(data):\n",
        "    obs = data['obs']\n",
        "    threshold_distance = 0.05  # Minimum distance to consider as \"too close\"\n",
        "    close_count = 0  # Counter for how many consecutive timesteps the blocks are close\n",
        "\n",
        "    for t in range(1, len(obs)):\n",
        "        block1_translation = obs[t][:2]\n",
        "        block2_translation = obs[t][3:5]\n",
        "        distance_between_blocks = np.linalg.norm(block1_translation - block2_translation)\n",
        "\n",
        "        # Check if the blocks are close together\n",
        "        if distance_between_blocks < threshold_distance:\n",
        "            close_count += 1\n",
        "            # If blocks are close for at least 10 consecutive timesteps, filter out this trajectory\n",
        "            if close_count >= 10:\n",
        "                return False\n",
        "        else:\n",
        "            close_count = 0\n",
        "\n",
        "    return True\n",
        "\n",
        "# Create filtered dataset1 using both first and second half datasets\n",
        "filtered_dataset1_first_half = FilteredBlockPushLowdimDataset(dataset_first_half, filter_block1_to_region1_or_region2)\n",
        "filtered_dataset1_second_half = FilteredBlockPushLowdimDataset(dataset_second_half, filter_block1_to_region1_or_region2)\n",
        "\n",
        "# Apply filter to remove trajectories where two blocks move together for dataset1\n",
        "filtered_dataset1_first_half = FilteredBlockPushLowdimDataset(filtered_dataset1_first_half, filter_no_two_blocks_moving_together)\n",
        "filtered_dataset1_second_half = FilteredBlockPushLowdimDataset(filtered_dataset1_second_half, filter_no_two_blocks_moving_together)\n",
        "\n",
        "# Combine filtered datasets to create dataset1\n",
        "dataset1 = torch.utils.data.ConcatDataset([filtered_dataset1_first_half, filtered_dataset1_second_half])\n",
        "print(\"Filtered dataset1 created containing trajectories involving pushing block1 to region1 or region2 and excluding cases where two blocks move together.\")\n",
        "\n",
        "# Create filtered dataset2 using both first and second half datasets\n",
        "filtered_dataset2_first_half = FilteredBlockPushLowdimDataset(dataset_first_half, filter_block1_or_block2_to_region1)\n",
        "filtered_dataset2_second_half = FilteredBlockPushLowdimDataset(dataset_second_half, filter_block1_or_block2_to_region1)\n",
        "\n",
        "# Apply filter to remove trajectories where two blocks move together for dataset2\n",
        "filtered_dataset2_first_half = FilteredBlockPushLowdimDataset(filtered_dataset2_first_half, filter_no_two_blocks_moving_together)\n",
        "filtered_dataset2_second_half = FilteredBlockPushLowdimDataset(filtered_dataset2_second_half, filter_no_two_blocks_moving_together)\n",
        "\n",
        "# Combine filtered datasets to create dataset2\n",
        "dataset2 = torch.utils.data.ConcatDataset([filtered_dataset2_first_half, filtered_dataset2_second_half])\n",
        "print(\"Filtered dataset2 created containing trajectories involving pushing block1 or block2 to region1.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization function for dataset1 and dataset2\n",
        "def visualize_trajectory(obs, actions, save_path=\"trajectory.mp4\"):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(-0.5, 1.5)\n",
        "    ax.set_ylim(-0.5, 1.5)\n",
        "    agent_patch = Rectangle((0, 0), 0.02, 0.02, fc='blue', label='Effector')\n",
        "    block_patch = Rectangle((0, 0), 0.03, 0.03, fc='red', label='Block')\n",
        "    block2_patch = Rectangle((0, 0), 0.03, 0.03, fc='green', label='Block 2')\n",
        "    target_patch = Circle((0, 0), 0.02, fc='orange', label='Target')\n",
        "    target2_patch = Circle((0, 0), 0.02, fc='purple', label='Target 2')\n",
        "    region1_circle = Circle((0, 0), 0.05, fill=False, edgecolor='orange', linestyle='--', label='Region 1 Boundary')\n",
        "    region2_circle = Circle((0, 0), 0.05, fill=False, edgecolor='purple', linestyle='--', label='Region 2 Boundary')\n",
        "    ax.add_patch(agent_patch)\n",
        "    ax.add_patch(block_patch)\n",
        "    ax.add_patch(block2_patch)\n",
        "    ax.add_patch(target_patch)\n",
        "    ax.add_patch(target2_patch)\n",
        "    ax.add_patch(region1_circle)\n",
        "    ax.add_patch(region2_circle)\n",
        "    plt.legend()\n",
        "\n",
        "    def update(frame):\n",
        "        block_translation = obs[frame][:2]\n",
        "        block_orientation = obs[frame][2]\n",
        "        block2_translation = obs[frame][3:5]\n",
        "        block2_orientation = obs[frame][5]\n",
        "        effector_translation = obs[frame][6:8]\n",
        "        effector_target_translation = obs[frame][8:10]\n",
        "        target_translation = obs[frame][10:12]\n",
        "        target_orientation = obs[frame][12]\n",
        "        target2_translation = obs[frame][13:15]\n",
        "        target2_orientation = obs[frame][15]\n",
        "        target_agent_x, target_agent_y = actions[frame][:2]\n",
        "        agent_patch.set_xy((effector_translation[0], effector_translation[1]))\n",
        "        block_patch.set_xy((block_translation[0], block_translation[1]))\n",
        "        block2_patch.set_xy((block2_translation[0], block2_translation[1]))\n",
        "        target_patch.set_center((target_translation[0], target_translation[1]))\n",
        "        target2_patch.set_center((target2_translation[0], target2_translation[1]))\n",
        "        region1_circle.set_center((target_translation[0], target_translation[1]))\n",
        "        region2_circle.set_center((target2_translation[0], target2_translation[1]))\n",
        "        ax.set_title(f\"Frame: {frame}, Target Agent: ({target_agent_x:.2f}, {target_agent_y:.2f}\" + \"\\n\"\n",
        "                     f\"Frame: {frame}, Current Agent: ({effector_translation[0]:.2f}, {effector_target_translation[1]:.2f})\")\n",
        "        return agent_patch, block_patch, block2_patch, target_patch, target2_patch, region1_circle, region2_circle\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(obs), blit=False, repeat=False)\n",
        "    ani.save(save_path, writer='ffmpeg', fps=5)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:07,494 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:07,496 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset1/dataset1_trajectory_640.mp4\n",
            "2024-11-26 19:11:20,199 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:20,201 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset1/dataset1_trajectory_688.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset1 trajectory 640 visualization saved as visualizations/dataset1/dataset1_trajectory_640.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:34,781 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:34,783 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset1/dataset1_trajectory_491.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset1 trajectory 688 visualization saved as visualizations/dataset1/dataset1_trajectory_688.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:46,969 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:46,971 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset2/dataset2_trajectory_34.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset1 trajectory 491 visualization saved as visualizations/dataset1/dataset1_trajectory_491.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:11:57,356 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:11:57,358 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset2/dataset2_trajectory_270.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset2 trajectory 34 visualization saved as visualizations/dataset2/dataset2_trajectory_34.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-26 19:12:07,682 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-26 19:12:07,684 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y visualizations/dataset2/dataset2_trajectory_1304.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset2 trajectory 270 visualization saved as visualizations/dataset2/dataset2_trajectory_270.mp4\n",
            "dataset2 trajectory 1304 visualization saved as visualizations/dataset2/dataset2_trajectory_1304.mp4\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def visualize_dataset(dataset, save_dir, dataset_name):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    indices = random.sample(range(len(dataset)), min(len(dataset), 3))  # Pick 10 random trajectories for visualization\n",
        "    for i in indices:  # Visualize up to 5 trajectories for verification\n",
        "        data = dataset[i]\n",
        "        obs = data['obs'].numpy()\n",
        "        actions = data['action'].numpy()\n",
        "        save_path = os.path.join(save_dir, f\"{dataset_name}_trajectory_{i}.mp4\")\n",
        "        visualize_trajectory(obs, actions, save_path)\n",
        "        print(f\"{dataset_name} trajectory {i} visualization saved as {save_path}\")\n",
        "\n",
        "# Visualize and save trajectories from dataset1 and dataset2\n",
        "visualize_dataset(dataset1, save_dir=\"visualizations/dataset1\", dataset_name=\"dataset1\")\n",
        "visualize_dataset(dataset2, save_dir=\"visualizations/dataset2\", dataset_name=\"dataset2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-27 11:23:42,037 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:23:42,039 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset1/dataset1_trajectory_0_block1_region2.mp4\n",
            "2024-11-27 11:23:57,921 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:23:57,924 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset1/dataset1_trajectory_7_block1_region1.mp4\n",
            "2024-11-27 11:24:12,710 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:24:12,713 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset2/dataset2_trajectory_0_block1_region1.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory categorization statistics:\n",
            "pushing_block1_to_region1: 621\n",
            "pushing_block2_to_region1: 0\n",
            "pushing_block1_to_region2: 674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-27 11:24:26,192 [INFO] Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
            "2024-11-27 11:24:26,193 [INFO] MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 5 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y ./visualizations/dataset2_trajectory_2_block2_region1.mp4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trajectory categorization statistics:\n",
            "pushing_block1_to_region1: 723\n",
            "pushing_block2_to_region1: 845\n",
            "pushing_block1_to_region2: 0\n"
          ]
        }
      ],
      "source": [
        "def categorize_trajectories(dataset, dataset_name):\n",
        "    categories = {\n",
        "        \"pushing_block1_to_region1\": 0,\n",
        "        \"pushing_block2_to_region1\": 0,\n",
        "        \"pushing_block1_to_region2\": 0\n",
        "    }\n",
        "    pushing_block1_to_region2_indices = []  # To record indices of incorrect cases\n",
        "\n",
        "    vs = [0,0,0]\n",
        "    for idx, data in enumerate(dataset):\n",
        "        obs = data['obs'].numpy()\n",
        "\n",
        "        region1_center = obs[0][10:12]\n",
        "        region2_center = obs[0][13:15]\n",
        "\n",
        "        block1_translation_initial = obs[0][:2]\n",
        "        block2_translation_initial = obs[0][3:5]\n",
        "        block1_translation_final = obs[-1][:2]\n",
        "        block2_translation_final = obs[-1][3:5]\n",
        "        region_radius = 0.05\n",
        "\n",
        "        # Check conditions for pushing block1 to region1\n",
        "        if (np.linalg.norm(block1_translation_initial - region1_center) > region_radius and\n",
        "                np.linalg.norm(block1_translation_final - region1_center) < region_radius):\n",
        "            categories[\"pushing_block1_to_region1\"] += 1\n",
        "\n",
        "            if vs[0] == 0:\n",
        "                actions = data['action'].numpy()\n",
        "                save_path = os.path.join(f'./visualizations/{dataset_name}', f\"{dataset_name}_trajectory_{idx}_block1_region1.mp4\")\n",
        "                visualize_trajectory(obs, actions, save_path)\n",
        "                vs[0] = 1\n",
        "\n",
        "        # Check conditions for pushing block2 to region1\n",
        "        elif (np.linalg.norm(block2_translation_initial - region1_center) > region_radius and\n",
        "                np.linalg.norm(block2_translation_final - region1_center) < region_radius):\n",
        "            categories[\"pushing_block2_to_region1\"] += 1\n",
        "\n",
        "            if vs[1] == 0:\n",
        "                actions = data['action'].numpy()\n",
        "                save_path = os.path.join('./visualizations', f\"{dataset_name}_trajectory_{idx}_block2_region1.mp4\")\n",
        "                visualize_trajectory(obs, actions, save_path)\n",
        "                vs[1] = 1\n",
        "\n",
        "        # Check conditions for pushing block1 to region2\n",
        "        elif (np.linalg.norm(block1_translation_initial - region2_center) > region_radius and\n",
        "                np.linalg.norm(block1_translation_final - region2_center) < region_radius):\n",
        "            categories[\"pushing_block1_to_region2\"] += 1\n",
        "\n",
        "            if vs[2] == 0:\n",
        "                actions = data['action'].numpy()\n",
        "                save_path = os.path.join(f'./visualizations/{dataset_name}', f\"{dataset_name}_trajectory_{idx}_block1_region2.mp4\")\n",
        "                visualize_trajectory(obs, actions, save_path)\n",
        "                vs[2] = 1\n",
        "\n",
        "    print(\"Trajectory categorization statistics:\")\n",
        "    for category, count in categories.items():\n",
        "        print(f\"{category}: {count}\")\n",
        "\n",
        "    # Print the indices of the incorrect cases for dataset2\n",
        "    if pushing_block1_to_region2_indices:\n",
        "        print(\"Indices of trajectories in dataset2 that incorrectly involve pushing block1 to region2:\")\n",
        "        print(pushing_block1_to_region2_indices)\n",
        "        obs = data['obs'].numpy()\n",
        "\n",
        "   \n",
        "\n",
        "# Categorize trajectories in dataset1 and dataset2\n",
        "categorize_trajectories(dataset1, 'dataset1')\n",
        "categorize_trajectories(dataset2, 'dataset2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 155 (char 154)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[81], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m action_horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#|o|o|                             observations: 2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#| |a|a|a|a|a|a|a|a|               actions executed: 8\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Reload dataset1 from Zarr with new parameters\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dataset1_new \u001b[38;5;241m=\u001b[39m \u001b[43mBlockPushLowdimDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzarr_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/training/block_pushing/dataset1.zarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_horizon\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_horizon\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Reload dataset2 from Zarr with new parameters\u001b[39;00m\n\u001b[1;32m     18\u001b[0m dataset2_new \u001b[38;5;241m=\u001b[39m BlockPushLowdimDataset(\n\u001b[1;32m     19\u001b[0m     zarr_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/training/block_pushing/dataset2.zarr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     horizon\u001b[38;5;241m=\u001b[39mpred_horizon,\n\u001b[1;32m     21\u001b[0m     pad_before\u001b[38;5;241m=\u001b[39mobs_horizon\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     22\u001b[0m     pad_after\u001b[38;5;241m=\u001b[39maction_horizon\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n",
            "File \u001b[0;32m~/git/research/Grounded-Lang-Compositionality/diffusion/diffusion_policy/diffusion_policy/dataset/blockpush_lowdim_dataset.py:25\u001b[0m, in \u001b[0;36mBlockPushLowdimDataset.__init__\u001b[0;34m(self, zarr_path, horizon, pad_before, pad_after, obs_key, action_key, obs_eef_target, use_manual_normalizer, seed, val_ratio)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     13\u001b[0m         zarr_path, \n\u001b[1;32m     14\u001b[0m         horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     23\u001b[0m         ):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mReplayBuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mobs_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     val_mask \u001b[38;5;241m=\u001b[39m get_val_mask(\n\u001b[1;32m     29\u001b[0m         n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mn_episodes, \n\u001b[1;32m     30\u001b[0m         val_ratio\u001b[38;5;241m=\u001b[39mval_ratio,\n\u001b[1;32m     31\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m     32\u001b[0m     train_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mval_mask\n",
            "File \u001b[0;32m~/git/research/Grounded-Lang-Compositionality/diffusion/diffusion_policy/diffusion_policy/common/replay_buffer.py:222\u001b[0m, in \u001b[0;36mReplayBuffer.copy_from_path\u001b[0;34m(cls, zarr_path, backend, store, keys, chunks, compressors, if_exists, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m group \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(zarr_path), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_from_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompressors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompressors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/git/research/Grounded-Lang-Compositionality/diffusion/diffusion_policy/diffusion_policy/common/replay_buffer.py:160\u001b[0m, in \u001b[0;36mReplayBuffer.copy_from_store\u001b[0;34m(cls, src_store, store, keys, chunks, compressors, if_exists, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# numpy backend\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m src_root[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    162\u001b[0m             meta[key] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(value)\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/_collections_abc.py:851\u001b[0m, in \u001b[0;36mItemsView.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 851\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (key, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/zarr/hierarchy.py:411\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    409\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_item_path(item)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m contains_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, path):\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_synchronizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m contains_group(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, path, explicit_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Group(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_only, path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    417\u001b[0m                  chunk_store\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_store, cache_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mcache,\n\u001b[1;32m    418\u001b[0m                  synchronizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronizer, zarr_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version)\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/zarr/core.py:203\u001b[0m, in \u001b[0;36mArray.__init__\u001b[0;34m(self, store, path, read_only, chunk_store, synchronizer, cache_metadata, cache_attrs, partial_decompress, write_empty_chunks, zarr_version)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_key_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hierarchy_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_key_suffix\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# initialize metadata\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# initialize attributes\u001b[39;00m\n\u001b[1;32m    206\u001b[0m akey \u001b[38;5;241m=\u001b[39m _prefix_to_attrs_key(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_prefix)\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/zarr/core.py:220\u001b[0m, in \u001b[0;36mArray._load_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"(Re)load metadata from store.\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_metadata_nosync\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     mkey \u001b[38;5;241m=\u001b[39m _prefix_to_array_key(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_prefix)\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/zarr/core.py:235\u001b[0m, in \u001b[0;36mArray._load_metadata_nosync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArrayNotFoundError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# decode and store metadata as instance members\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadata_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_array_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;241m=\u001b[39m meta\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/zarr/meta.py:110\u001b[0m, in \u001b[0;36mMetadata2.decode_array_metadata\u001b[0;34m(cls, s)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_array_metadata\u001b[39m(\u001b[38;5;28mcls\u001b[39m, s: Union[MappingType, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MappingType[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 110\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# check metadata format\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     zarr_format \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzarr_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/zarr/meta.py:104\u001b[0m, in \u001b[0;36mMetadata2.parse_metadata\u001b[0;34m(cls, s)\u001b[0m\n\u001b[1;32m    100\u001b[0m     meta \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# assume metadata needs to be parsed as JSON\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mjson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m meta\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/zarr/util.py:56\u001b[0m, in \u001b[0;36mjson_loads\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson_loads\u001b[39m(s: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read JSON in a consistent way.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mascii\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 155 (char 154)"
          ]
        }
      ],
      "source": [
        "# parameters\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "#|o|o|                             observations: 2\n",
        "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
        "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
        "\n",
        "# Reload dataset1 from Zarr with new parameters\n",
        "dataset1_new = BlockPushLowdimDataset(\n",
        "    zarr_path=\"./data/training/block_pushing/dataset1.zarr\",\n",
        "    horizon=pred_horizon,\n",
        "    pad_before=obs_horizon-1,\n",
        "    pad_after=action_horizon-1\n",
        ")\n",
        "\n",
        "# Reload dataset2 from Zarr with new parameters\n",
        "dataset2_new = BlockPushLowdimDataset(\n",
        "    zarr_path=\"./data/training/block_pushing/dataset2.zarr\",\n",
        "    horizon=pred_horizon,\n",
        "    pad_before=obs_horizon-1,\n",
        "    pad_after=action_horizon-1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "X-XRB_g3vsgf"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Network**\n",
        "#@markdown\n",
        "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
        "#@markdown as the noies prediction network\n",
        "#@markdown\n",
        "#@markdown Components\n",
        "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
        "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
        "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
        "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
        "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
        "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
        "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self,\n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level.\n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        cond_dim = dsed + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "            sample: torch.Tensor,\n",
        "            timestep: Union[torch.Tensor, float, int],\n",
        "            global_cond=None):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        global_feature = self.diffusion_step_encoder(timesteps)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "\n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4APZkqh336-M",
        "outputId": "51fbbfb0-deaf-48bd-cf4b-dc7f8f4ff246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 6.566861e+07\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Network Demo**\n",
        "\n",
        "# observation and action dimensions corrsponding to\n",
        "# the output of PushTEnv\n",
        "# obs_dim = 5\n",
        "obs_dim = 16\n",
        "action_dim = 2\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# example inputs\n",
        "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
        "diffusion_iter = torch.zeros((1,))\n",
        "\n",
        "# the noise prediction network\n",
        "# takes noisy action, diffusion iteration and observation as input\n",
        "# predicts the noise added to action\n",
        "noise = noise_pred_net(\n",
        "    sample=noised_action,\n",
        "    timestep=diffusion_iter,\n",
        "    global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "# illustration of removing noise\n",
        "# the actual noise removal is performed by NoiseScheduler\n",
        "# and is dependent on the diffusion noise schedule\n",
        "denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = noise_pred_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c29b0972b22d4dc59844ea958abf7bd1",
            "df7b2e7c39944341be1a091331fd7cc6",
            "0d687f5624cb4871ad167ebcbcb4c148",
            "82a5a8a6be3a41209de270d5072838a2",
            "6d4dfa99470e40559bebd6689305f155",
            "0964ed28f2794bdf91e2e2756aae3bc4",
            "1939a1c3cc7a498fa5f05ac737b0af99",
            "116ecf0deb6e44faadb594f2f982c4c2",
            "2db415245cbb44c8ab5c208be328fc16",
            "ed0c7c76b76d40c9b25ca92abb00cf34",
            "3a5463e9f9864ca2b52fe9cd28f4a8b8",
            "5352c2178612408aa84a3732d98a62ea",
            "f451b98692b64b54a0e8a3643a9bf992",
            "36c1a61163804f9a825638c6c8d962ed",
            "c2d84f324ecd4789b9b3420591f6186d",
            "6d4ec5c2f9624d398f0d9fc27d84cadb",
            "019444645b164b92a8da32e94a443d7e",
            "83bd2cb0ca534108804cdc298d8b26c6",
            "a26e0dce0a86491db71a731f570f1a80",
            "fe61754736d04d539c3f941049b5922a",
            "657a261a272d466e8bfd532279a401a1",
            "5b6eaafac1574f7481e3bb2e20e598b3"
          ]
        },
        "id": "93E9RdnR4D8v",
        "outputId": "51c50846-28b7-408c-c1d9-cd34154b57f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   1%|          | 1/100 [00:59<1:37:54, 59.34s/it, loss=0.0598]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m ema\u001b[38;5;241m.\u001b[39mstep(noise_pred_net\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# logging\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m loss_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss_cpu)\n\u001b[1;32m     83\u001b[0m tepoch\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss_cpu)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@markdown ### **Training**\n",
        "#@markdown\n",
        "#@markdown Takes about an hour. If you don't want to wait, skip to the next cell\n",
        "#@markdown to load pre-trained weights\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "# Exponential Moving Average\n",
        "# accelerates training and improves stability\n",
        "# holds a copy of the model weights\n",
        "ema = EMAModel(\n",
        "    parameters=noise_pred_net.parameters(),\n",
        "    power=0.75)\n",
        "\n",
        "# Standard ADAM optimizer\n",
        "# Note that EMA parametesr are not optimized\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=noise_pred_net.parameters(),\n",
        "    lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "# Cosine LR schedule with linear warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "    # epoch loop\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        # batch loop\n",
        "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                # data normalized in dataset\n",
        "                # device transfer\n",
        "                nobs = nbatch['obs'].to(device)\n",
        "                naction = nbatch['action'].to(device)\n",
        "                B = nobs.shape[0]\n",
        "\n",
        "                # observation as FiLM conditioning\n",
        "                # (B, obs_horizon, obs_dim)\n",
        "                obs_cond = nobs[:,:obs_horizon,:]\n",
        "                # (B, obs_horizon * obs_dim)\n",
        "                obs_cond = obs_cond.flatten(start_dim=1)\n",
        "\n",
        "                # sample noise to add to actions\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                # sample a diffusion iteration for each data point\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps,\n",
        "                    (B,), device=device\n",
        "                ).long()\n",
        "\n",
        "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_actions = noise_scheduler.add_noise(\n",
        "                    naction, noise, timesteps)\n",
        "\n",
        "                # predict the noise residual\n",
        "                noise_pred = noise_pred_net(\n",
        "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
        "\n",
        "                # L2 loss\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                # optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                # step lr scheduler every batch\n",
        "                # this is different from standard pytorch behavior\n",
        "                lr_scheduler.step()\n",
        "\n",
        "                # update Exponential Moving Average of the model weights\n",
        "                ema.step(noise_pred_net.parameters())\n",
        "\n",
        "                # logging\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "# Weights of the EMA model\n",
        "# is used for inference\n",
        "ema_noise_pred_net = noise_pred_net\n",
        "ema.copy_to(ema_noise_pred_net.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training code for two separate models\n",
        "num_epochs = 100\n",
        "\n",
        "# Training for model 1\n",
        "ema1 = EMAModel(parameters=noise_pred_net.parameters(), power=0.75)\n",
        "optimizer1 = torch.optim.AdamW(params=noise_pred_net.parameters(), lr=1e-4, weight_decay=1e-6)\n",
        "lr_scheduler1 = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer1,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(filtered_dataloader1) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Model 1 Training Epoch') as tglobal:\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        with tqdm(filtered_dataloader1, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                nobs = nbatch['obs'].to(device)\n",
        "                naction = nbatch['action'].to(device)\n",
        "                B = nobs.shape[0]\n",
        "                obs_cond = nobs[:, :obs_horizon, :].flatten(start_dim=1)\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (B,), device=device).long()\n",
        "                noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
        "                noise_pred = noise_pred_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "                loss.backward()\n",
        "                optimizer1.step()\n",
        "                optimizer1.zero_grad()\n",
        "                lr_scheduler1.step()\n",
        "                ema1.step(noise_pred_net.parameters())\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "# Training for model 2\n",
        "ema2 = EMAModel(parameters=noise_pred_net.parameters(), power=0.75)\n",
        "optimizer2 = torch.optim.AdamW(params=noise_pred_net.parameters(), lr=1e-4, weight_decay=1e-6)\n",
        "lr_scheduler2 = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer2,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(filtered_dataloader2) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Model 2 Training Epoch') as tglobal:\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        with tqdm(filtered_dataloader2, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                nobs = nbatch['obs'].to(device)\n",
        "                naction = nbatch['action'].to(device)\n",
        "                B = nobs.shape[0]\n",
        "                obs_cond = nobs[:, :obs_horizon, :].flatten(start_dim=1)\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (B,), device=device).long()\n",
        "                noisy_actions = noise_scheduler.add_noise(naction, noise, timesteps)\n",
        "                noise_pred = noise_pred_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "                loss.backward()\n",
        "                optimizer2.step()\n",
        "                optimizer2.zero_grad()\n",
        "                lr_scheduler2.step()\n",
        "                ema2.step(noise_pred_net.parameters())\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "# Weights of the EMA model are used for inference\n",
        "ema_noise_pred_net1 = noise_pred_net\n",
        "ema1.copy_to(ema_noise_pred_net1.parameters())\n",
        "\n",
        "ema_noise_pred_net2 = noise_pred_net\n",
        "ema2.copy_to(ema_noise_pred_net2.parameters())\n",
        "\n",
        "print(\"Training of both models completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F3hUbIuxGdO",
        "outputId": "1c77956b-0006-403d-a9af-ce41def182bc"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ConditionalUnet1D:\n\tMissing key(s) in state_dict: \"mid_modules.0.blocks.0.block.0.weight\", \"mid_modules.0.blocks.0.block.0.bias\", \"mid_modules.0.blocks.0.block.1.weight\", \"mid_modules.0.blocks.0.block.1.bias\", \"mid_modules.0.blocks.1.block.0.weight\", \"mid_modules.0.blocks.1.block.0.bias\", \"mid_modules.0.blocks.1.block.1.weight\", \"mid_modules.0.blocks.1.block.1.bias\", \"mid_modules.0.cond_encoder.1.weight\", \"mid_modules.0.cond_encoder.1.bias\", \"mid_modules.1.blocks.0.block.0.weight\", \"mid_modules.1.blocks.0.block.0.bias\", \"mid_modules.1.blocks.0.block.1.weight\", \"mid_modules.1.blocks.0.block.1.bias\", \"mid_modules.1.blocks.1.block.0.weight\", \"mid_modules.1.blocks.1.block.0.bias\", \"mid_modules.1.blocks.1.block.1.weight\", \"mid_modules.1.blocks.1.block.1.bias\", \"mid_modules.1.cond_encoder.1.weight\", \"mid_modules.1.cond_encoder.1.bias\", \"diffusion_step_encoder.1.weight\", \"diffusion_step_encoder.1.bias\", \"diffusion_step_encoder.3.weight\", \"diffusion_step_encoder.3.bias\", \"up_modules.0.0.blocks.0.block.0.weight\", \"up_modules.0.0.blocks.0.block.0.bias\", \"up_modules.0.0.blocks.0.block.1.weight\", \"up_modules.0.0.blocks.0.block.1.bias\", \"up_modules.0.0.blocks.1.block.0.weight\", \"up_modules.0.0.blocks.1.block.0.bias\", \"up_modules.0.0.blocks.1.block.1.weight\", \"up_modules.0.0.blocks.1.block.1.bias\", \"up_modules.0.0.cond_encoder.1.weight\", \"up_modules.0.0.cond_encoder.1.bias\", \"up_modules.0.0.residual_conv.weight\", \"up_modules.0.0.residual_conv.bias\", \"up_modules.0.1.blocks.0.block.0.weight\", \"up_modules.0.1.blocks.0.block.0.bias\", \"up_modules.0.1.blocks.0.block.1.weight\", \"up_modules.0.1.blocks.0.block.1.bias\", \"up_modules.0.1.blocks.1.block.0.weight\", \"up_modules.0.1.blocks.1.block.0.bias\", \"up_modules.0.1.blocks.1.block.1.weight\", \"up_modules.0.1.blocks.1.block.1.bias\", \"up_modules.0.1.cond_encoder.1.weight\", \"up_modules.0.1.cond_encoder.1.bias\", \"up_modules.0.2.conv.weight\", \"up_modules.0.2.conv.bias\", \"up_modules.1.0.blocks.0.block.0.weight\", \"up_modules.1.0.blocks.0.block.0.bias\", \"up_modules.1.0.blocks.0.block.1.weight\", \"up_modules.1.0.blocks.0.block.1.bias\", \"up_modules.1.0.blocks.1.block.0.weight\", \"up_modules.1.0.blocks.1.block.0.bias\", \"up_modules.1.0.blocks.1.block.1.weight\", \"up_modules.1.0.blocks.1.block.1.bias\", \"up_modules.1.0.cond_encoder.1.weight\", \"up_modules.1.0.cond_encoder.1.bias\", \"up_modules.1.0.residual_conv.weight\", \"up_modules.1.0.residual_conv.bias\", \"up_modules.1.1.blocks.0.block.0.weight\", \"up_modules.1.1.blocks.0.block.0.bias\", \"up_modules.1.1.blocks.0.block.1.weight\", \"up_modules.1.1.blocks.0.block.1.bias\", \"up_modules.1.1.blocks.1.block.0.weight\", \"up_modules.1.1.blocks.1.block.0.bias\", \"up_modules.1.1.blocks.1.block.1.weight\", \"up_modules.1.1.blocks.1.block.1.bias\", \"up_modules.1.1.cond_encoder.1.weight\", \"up_modules.1.1.cond_encoder.1.bias\", \"up_modules.1.2.conv.weight\", \"up_modules.1.2.conv.bias\", \"down_modules.0.0.blocks.0.block.0.weight\", \"down_modules.0.0.blocks.0.block.0.bias\", \"down_modules.0.0.blocks.0.block.1.weight\", \"down_modules.0.0.blocks.0.block.1.bias\", \"down_modules.0.0.blocks.1.block.0.weight\", \"down_modules.0.0.blocks.1.block.0.bias\", \"down_modules.0.0.blocks.1.block.1.weight\", \"down_modules.0.0.blocks.1.block.1.bias\", \"down_modules.0.0.cond_encoder.1.weight\", \"down_modules.0.0.cond_encoder.1.bias\", \"down_modules.0.0.residual_conv.weight\", \"down_modules.0.0.residual_conv.bias\", \"down_modules.0.1.blocks.0.block.0.weight\", \"down_modules.0.1.blocks.0.block.0.bias\", \"down_modules.0.1.blocks.0.block.1.weight\", \"down_modules.0.1.blocks.0.block.1.bias\", \"down_modules.0.1.blocks.1.block.0.weight\", \"down_modules.0.1.blocks.1.block.0.bias\", \"down_modules.0.1.blocks.1.block.1.weight\", \"down_modules.0.1.blocks.1.block.1.bias\", \"down_modules.0.1.cond_encoder.1.weight\", \"down_modules.0.1.cond_encoder.1.bias\", \"down_modules.0.2.conv.weight\", \"down_modules.0.2.conv.bias\", \"down_modules.1.0.blocks.0.block.0.weight\", \"down_modules.1.0.blocks.0.block.0.bias\", \"down_modules.1.0.blocks.0.block.1.weight\", \"down_modules.1.0.blocks.0.block.1.bias\", \"down_modules.1.0.blocks.1.block.0.weight\", \"down_modules.1.0.blocks.1.block.0.bias\", \"down_modules.1.0.blocks.1.block.1.weight\", \"down_modules.1.0.blocks.1.block.1.bias\", \"down_modules.1.0.cond_encoder.1.weight\", \"down_modules.1.0.cond_encoder.1.bias\", \"down_modules.1.0.residual_conv.weight\", \"down_modules.1.0.residual_conv.bias\", \"down_modules.1.1.blocks.0.block.0.weight\", \"down_modules.1.1.blocks.0.block.0.bias\", \"down_modules.1.1.blocks.0.block.1.weight\", \"down_modules.1.1.blocks.0.block.1.bias\", \"down_modules.1.1.blocks.1.block.0.weight\", \"down_modules.1.1.blocks.1.block.0.bias\", \"down_modules.1.1.blocks.1.block.1.weight\", \"down_modules.1.1.blocks.1.block.1.bias\", \"down_modules.1.1.cond_encoder.1.weight\", \"down_modules.1.1.cond_encoder.1.bias\", \"down_modules.1.2.conv.weight\", \"down_modules.1.2.conv.bias\", \"down_modules.2.0.blocks.0.block.0.weight\", \"down_modules.2.0.blocks.0.block.0.bias\", \"down_modules.2.0.blocks.0.block.1.weight\", \"down_modules.2.0.blocks.0.block.1.bias\", \"down_modules.2.0.blocks.1.block.0.weight\", \"down_modules.2.0.blocks.1.block.0.bias\", \"down_modules.2.0.blocks.1.block.1.weight\", \"down_modules.2.0.blocks.1.block.1.bias\", \"down_modules.2.0.cond_encoder.1.weight\", \"down_modules.2.0.cond_encoder.1.bias\", \"down_modules.2.0.residual_conv.weight\", \"down_modules.2.0.residual_conv.bias\", \"down_modules.2.1.blocks.0.block.0.weight\", \"down_modules.2.1.blocks.0.block.0.bias\", \"down_modules.2.1.blocks.0.block.1.weight\", \"down_modules.2.1.blocks.0.block.1.bias\", \"down_modules.2.1.blocks.1.block.0.weight\", \"down_modules.2.1.blocks.1.block.0.bias\", \"down_modules.2.1.blocks.1.block.1.weight\", \"down_modules.2.1.blocks.1.block.1.bias\", \"down_modules.2.1.cond_encoder.1.weight\", \"down_modules.2.1.cond_encoder.1.bias\", \"final_conv.0.block.0.weight\", \"final_conv.0.block.0.bias\", \"final_conv.0.block.1.weight\", \"final_conv.0.block.1.bias\", \"final_conv.1.weight\", \"final_conv.1.bias\". \n\tUnexpected key(s) in state_dict: \"cfg\", \"state_dicts\", \"pickles\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m   state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m   ema_noise_pred_net \u001b[38;5;241m=\u001b[39m noise_pred_net\n\u001b[0;32m---> 15\u001b[0m   \u001b[43mema_noise_pred_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPretrained weights loaded.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConditionalUnet1D:\n\tMissing key(s) in state_dict: \"mid_modules.0.blocks.0.block.0.weight\", \"mid_modules.0.blocks.0.block.0.bias\", \"mid_modules.0.blocks.0.block.1.weight\", \"mid_modules.0.blocks.0.block.1.bias\", \"mid_modules.0.blocks.1.block.0.weight\", \"mid_modules.0.blocks.1.block.0.bias\", \"mid_modules.0.blocks.1.block.1.weight\", \"mid_modules.0.blocks.1.block.1.bias\", \"mid_modules.0.cond_encoder.1.weight\", \"mid_modules.0.cond_encoder.1.bias\", \"mid_modules.1.blocks.0.block.0.weight\", \"mid_modules.1.blocks.0.block.0.bias\", \"mid_modules.1.blocks.0.block.1.weight\", \"mid_modules.1.blocks.0.block.1.bias\", \"mid_modules.1.blocks.1.block.0.weight\", \"mid_modules.1.blocks.1.block.0.bias\", \"mid_modules.1.blocks.1.block.1.weight\", \"mid_modules.1.blocks.1.block.1.bias\", \"mid_modules.1.cond_encoder.1.weight\", \"mid_modules.1.cond_encoder.1.bias\", \"diffusion_step_encoder.1.weight\", \"diffusion_step_encoder.1.bias\", \"diffusion_step_encoder.3.weight\", \"diffusion_step_encoder.3.bias\", \"up_modules.0.0.blocks.0.block.0.weight\", \"up_modules.0.0.blocks.0.block.0.bias\", \"up_modules.0.0.blocks.0.block.1.weight\", \"up_modules.0.0.blocks.0.block.1.bias\", \"up_modules.0.0.blocks.1.block.0.weight\", \"up_modules.0.0.blocks.1.block.0.bias\", \"up_modules.0.0.blocks.1.block.1.weight\", \"up_modules.0.0.blocks.1.block.1.bias\", \"up_modules.0.0.cond_encoder.1.weight\", \"up_modules.0.0.cond_encoder.1.bias\", \"up_modules.0.0.residual_conv.weight\", \"up_modules.0.0.residual_conv.bias\", \"up_modules.0.1.blocks.0.block.0.weight\", \"up_modules.0.1.blocks.0.block.0.bias\", \"up_modules.0.1.blocks.0.block.1.weight\", \"up_modules.0.1.blocks.0.block.1.bias\", \"up_modules.0.1.blocks.1.block.0.weight\", \"up_modules.0.1.blocks.1.block.0.bias\", \"up_modules.0.1.blocks.1.block.1.weight\", \"up_modules.0.1.blocks.1.block.1.bias\", \"up_modules.0.1.cond_encoder.1.weight\", \"up_modules.0.1.cond_encoder.1.bias\", \"up_modules.0.2.conv.weight\", \"up_modules.0.2.conv.bias\", \"up_modules.1.0.blocks.0.block.0.weight\", \"up_modules.1.0.blocks.0.block.0.bias\", \"up_modules.1.0.blocks.0.block.1.weight\", \"up_modules.1.0.blocks.0.block.1.bias\", \"up_modules.1.0.blocks.1.block.0.weight\", \"up_modules.1.0.blocks.1.block.0.bias\", \"up_modules.1.0.blocks.1.block.1.weight\", \"up_modules.1.0.blocks.1.block.1.bias\", \"up_modules.1.0.cond_encoder.1.weight\", \"up_modules.1.0.cond_encoder.1.bias\", \"up_modules.1.0.residual_conv.weight\", \"up_modules.1.0.residual_conv.bias\", \"up_modules.1.1.blocks.0.block.0.weight\", \"up_modules.1.1.blocks.0.block.0.bias\", \"up_modules.1.1.blocks.0.block.1.weight\", \"up_modules.1.1.blocks.0.block.1.bias\", \"up_modules.1.1.blocks.1.block.0.weight\", \"up_modules.1.1.blocks.1.block.0.bias\", \"up_modules.1.1.blocks.1.block.1.weight\", \"up_modules.1.1.blocks.1.block.1.bias\", \"up_modules.1.1.cond_encoder.1.weight\", \"up_modules.1.1.cond_encoder.1.bias\", \"up_modules.1.2.conv.weight\", \"up_modules.1.2.conv.bias\", \"down_modules.0.0.blocks.0.block.0.weight\", \"down_modules.0.0.blocks.0.block.0.bias\", \"down_modules.0.0.blocks.0.block.1.weight\", \"down_modules.0.0.blocks.0.block.1.bias\", \"down_modules.0.0.blocks.1.block.0.weight\", \"down_modules.0.0.blocks.1.block.0.bias\", \"down_modules.0.0.blocks.1.block.1.weight\", \"down_modules.0.0.blocks.1.block.1.bias\", \"down_modules.0.0.cond_encoder.1.weight\", \"down_modules.0.0.cond_encoder.1.bias\", \"down_modules.0.0.residual_conv.weight\", \"down_modules.0.0.residual_conv.bias\", \"down_modules.0.1.blocks.0.block.0.weight\", \"down_modules.0.1.blocks.0.block.0.bias\", \"down_modules.0.1.blocks.0.block.1.weight\", \"down_modules.0.1.blocks.0.block.1.bias\", \"down_modules.0.1.blocks.1.block.0.weight\", \"down_modules.0.1.blocks.1.block.0.bias\", \"down_modules.0.1.blocks.1.block.1.weight\", \"down_modules.0.1.blocks.1.block.1.bias\", \"down_modules.0.1.cond_encoder.1.weight\", \"down_modules.0.1.cond_encoder.1.bias\", \"down_modules.0.2.conv.weight\", \"down_modules.0.2.conv.bias\", \"down_modules.1.0.blocks.0.block.0.weight\", \"down_modules.1.0.blocks.0.block.0.bias\", \"down_modules.1.0.blocks.0.block.1.weight\", \"down_modules.1.0.blocks.0.block.1.bias\", \"down_modules.1.0.blocks.1.block.0.weight\", \"down_modules.1.0.blocks.1.block.0.bias\", \"down_modules.1.0.blocks.1.block.1.weight\", \"down_modules.1.0.blocks.1.block.1.bias\", \"down_modules.1.0.cond_encoder.1.weight\", \"down_modules.1.0.cond_encoder.1.bias\", \"down_modules.1.0.residual_conv.weight\", \"down_modules.1.0.residual_conv.bias\", \"down_modules.1.1.blocks.0.block.0.weight\", \"down_modules.1.1.blocks.0.block.0.bias\", \"down_modules.1.1.blocks.0.block.1.weight\", \"down_modules.1.1.blocks.0.block.1.bias\", \"down_modules.1.1.blocks.1.block.0.weight\", \"down_modules.1.1.blocks.1.block.0.bias\", \"down_modules.1.1.blocks.1.block.1.weight\", \"down_modules.1.1.blocks.1.block.1.bias\", \"down_modules.1.1.cond_encoder.1.weight\", \"down_modules.1.1.cond_encoder.1.bias\", \"down_modules.1.2.conv.weight\", \"down_modules.1.2.conv.bias\", \"down_modules.2.0.blocks.0.block.0.weight\", \"down_modules.2.0.blocks.0.block.0.bias\", \"down_modules.2.0.blocks.0.block.1.weight\", \"down_modules.2.0.blocks.0.block.1.bias\", \"down_modules.2.0.blocks.1.block.0.weight\", \"down_modules.2.0.blocks.1.block.0.bias\", \"down_modules.2.0.blocks.1.block.1.weight\", \"down_modules.2.0.blocks.1.block.1.bias\", \"down_modules.2.0.cond_encoder.1.weight\", \"down_modules.2.0.cond_encoder.1.bias\", \"down_modules.2.0.residual_conv.weight\", \"down_modules.2.0.residual_conv.bias\", \"down_modules.2.1.blocks.0.block.0.weight\", \"down_modules.2.1.blocks.0.block.0.bias\", \"down_modules.2.1.blocks.0.block.1.weight\", \"down_modules.2.1.blocks.0.block.1.bias\", \"down_modules.2.1.blocks.1.block.0.weight\", \"down_modules.2.1.blocks.1.block.0.bias\", \"down_modules.2.1.blocks.1.block.1.weight\", \"down_modules.2.1.blocks.1.block.1.bias\", \"down_modules.2.1.cond_encoder.1.weight\", \"down_modules.2.1.cond_encoder.1.bias\", \"final_conv.0.block.0.weight\", \"final_conv.0.block.0.bias\", \"final_conv.0.block.1.weight\", \"final_conv.0.block.1.bias\", \"final_conv.1.weight\", \"final_conv.1.bias\". \n\tUnexpected key(s) in state_dict: \"cfg\", \"state_dicts\", \"pickles\". "
          ]
        }
      ],
      "source": [
        "#@markdown ### **Loading Pretrained Checkpoint**\n",
        "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
        "\n",
        "load_pretrained = True\n",
        "if load_pretrained:\n",
        "  # ckpt_path = \"pusht_state_100ep.ckpt\"\n",
        "  ckpt_path = \"data/experiments/low_dim/block_pushing/diffusion_policy_cnn/train_0/checkpoints/latest.ckpt\"\n",
        "  # if not os.path.isfile(ckpt_path):\n",
        "      # id = \"1mHDr_DEZSdiGo9yecL50BBQYzR8Fjhl_&confirm=t\"\n",
        "      # gdown.download(id=id, output=ckpt_path, quiet=False)\n",
        "\n",
        "  # state_dict = torch.load(ckpt_path, map_location='cuda')\n",
        "  state_dict = torch.load(ckpt_path, map_location='cuda', weights_only=False)\n",
        "  ema_noise_pred_net = noise_pred_net\n",
        "  ema_noise_pred_net.load_state_dict(state_dict)\n",
        "  print('Pretrained weights loaded.')\n",
        "else:\n",
        "  print(\"Skipped pretrained weight loading.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-15 15:40:30,008 [INFO] number of parameters: 6.566861e+07\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DiffusionUnetLowdimPolicy(\n",
              "  (model): ConditionalUnet1D(\n",
              "    (mid_modules): ModuleList(\n",
              "      (0-1): 2 x ConditionalResidualBlock1D(\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x Conv1dBlock(\n",
              "            (block): Sequential(\n",
              "              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "              (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
              "              (2): Mish()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (cond_encoder): Sequential(\n",
              "          (0): Mish()\n",
              "          (1): Linear(in_features=288, out_features=2048, bias=True)\n",
              "          (2): Rearrange('batch t -> batch t 1')\n",
              "        )\n",
              "        (residual_conv): Identity()\n",
              "      )\n",
              "    )\n",
              "    (diffusion_step_encoder): Sequential(\n",
              "      (0): SinusoidalPosEmb()\n",
              "      (1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "      (2): Mish()\n",
              "      (3): Linear(in_features=1024, out_features=256, bias=True)\n",
              "    )\n",
              "    (up_modules): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(2048, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "            (1): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=1024, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "        )\n",
              "        (1): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=1024, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Identity()\n",
              "        )\n",
              "        (2): Upsample1d(\n",
              "          (conv): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(1024, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "            (1): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=512, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
              "        )\n",
              "        (1): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=512, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Identity()\n",
              "        )\n",
              "        (2): Upsample1d(\n",
              "          (conv): ConvTranspose1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (down_modules): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(2, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "            (1): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=512, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Conv1d(2, 256, kernel_size=(1,), stride=(1,))\n",
              "        )\n",
              "        (1): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=512, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Identity()\n",
              "        )\n",
              "        (2): Downsample1d(\n",
              "          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "            (1): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=1024, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
              "        )\n",
              "        (1): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=1024, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Identity()\n",
              "        )\n",
              "        (2): Downsample1d(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "            (1): Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=2048, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "        )\n",
              "        (1): ConditionalResidualBlock1D(\n",
              "          (blocks): ModuleList(\n",
              "            (0-1): 2 x Conv1dBlock(\n",
              "              (block): Sequential(\n",
              "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
              "                (2): Mish()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (cond_encoder): Sequential(\n",
              "            (0): Mish()\n",
              "            (1): Linear(in_features=288, out_features=2048, bias=True)\n",
              "            (2): Rearrange('batch t -> batch t 1')\n",
              "          )\n",
              "          (residual_conv): Identity()\n",
              "        )\n",
              "        (2): Identity()\n",
              "      )\n",
              "    )\n",
              "    (final_conv): Sequential(\n",
              "      (0): Conv1dBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "          (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "          (2): Mish()\n",
              "        )\n",
              "      )\n",
              "      (1): Conv1d(256, 2, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "  )\n",
              "  (mask_generator): LowdimMaskGenerator()\n",
              "  (normalizer): LinearNormalizer(\n",
              "    (params_dict): ParameterDict(\n",
              "        (obs): Object of type: ParameterDict\n",
              "        (action): Object of type: ParameterDict\n",
              "      (obs): ParameterDict(\n",
              "          (offset): Parameter containing: [torch.cuda.FloatTensor of size 16 (cuda:0)]\n",
              "          (scale): Parameter containing: [torch.cuda.FloatTensor of size 16 (cuda:0)]\n",
              "          (input_stats): Object of type: ParameterDict\n",
              "        (input_stats): ParameterDict(\n",
              "            (max): Parameter containing: [torch.cuda.FloatTensor of size 16 (cuda:0)]\n",
              "            (mean): Parameter containing: [torch.cuda.FloatTensor of size 16 (cuda:0)]\n",
              "            (min): Parameter containing: [torch.cuda.FloatTensor of size 16 (cuda:0)]\n",
              "            (std): Parameter containing: [torch.cuda.FloatTensor of size 16 (cuda:0)]\n",
              "        )\n",
              "      )\n",
              "      (action): ParameterDict(\n",
              "          (offset): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
              "          (scale): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
              "          (input_stats): Object of type: ParameterDict\n",
              "        (input_stats): ParameterDict(\n",
              "            (max): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
              "            (mean): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
              "            (min): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
              "            (std): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import dill\n",
        "import hydra\n",
        "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
        "\n",
        "# load checkpoint\n",
        "ckpt_path = \"data/experiments/low_dim/block_pushing/diffusion_policy_cnn/train_2/checkpoints/latest.ckpt\"\n",
        "output_dir = \"data/trash\"\n",
        "payload = torch.load(open(ckpt_path, 'rb'), pickle_module=dill) \n",
        "cfg = payload['cfg']\n",
        "cls = hydra.utils.get_class(cfg._target_)\n",
        "workspace = cls(cfg, output_dir=output_dir)\n",
        "workspace: BaseWorkspace\n",
        "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
        "\n",
        "\n",
        "# get policy from workspace\n",
        "policy = workspace.model\n",
        "if cfg.training.use_ema:\n",
        "    policy = workspace.ema_model\n",
        "\n",
        "device = torch.device(device)\n",
        "policy.to(device)\n",
        "policy.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "1c17844c76d84675aa1c6e1a973308c8",
            "aa9489756ac24ba0b523f04a3352a09f",
            "51ff5f151bc9475fa3a5c54f8b60a50e",
            "58656f1d31ac45d3aaef45aa22f69899",
            "b9f98d2d7c624a8eae13fa5b422739ce",
            "b72ff2e7020f447fb492c1e9f69a0174",
            "567e3614e80743e5ae1f8e3c75a65b45",
            "187c8fd96f79429bb0752103d5998401",
            "17d07e24261b4627b1160beeb59c5636",
            "269772eda6cc4449bbba9e1e684b442c",
            "7cad813857df45f3ad617c74c68a619e"
          ]
        },
        "id": "OyLjlNQk5nr9",
        "outputId": "a104d1d1-f25a-456d-cac9-520ff50f455f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sfchen/miniforge3/envs/robodiff/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argv[0]=\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval BlockPushLowdim:   0%|          | 0/200 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'collections.OrderedDict' object has no attribute 'astype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m         np_obs_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 43\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     44\u001b[0m         }\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# device transfer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         obs_dict \u001b[38;5;241m=\u001b[39m dict_apply(np_obs_dict, \n\u001b[1;32m     47\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     48\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'astype'"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Inference**\n",
        "\n",
        "from typing import Dict, Callable, List\n",
        "# limit enviornment interaction to 200 steps before termination\n",
        "max_steps = 200\n",
        "# env = PushTEnv()\n",
        "env = BlockPushMultimodal()\n",
        "# use a seed >200 to avoid initial states seen in the training dataset\n",
        "env.seed(100000)\n",
        "\n",
        "# get first observation\n",
        "obs = env.reset()\n",
        "\n",
        "policy.reset()\n",
        "\n",
        "# keep a queue of last 2 steps of observations\n",
        "obs_deque = collections.deque(\n",
        "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
        "# save visualization and rewards\n",
        "imgs = [env.render(mode='rgb_array')]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "\n",
        "def dict_apply(\n",
        "        x: Dict[str, torch.Tensor], \n",
        "        func: Callable[[torch.Tensor], torch.Tensor]\n",
        "        ) -> Dict[str, torch.Tensor]:\n",
        "    result = dict()\n",
        "    for key, value in x.items():\n",
        "        if isinstance(value, dict):\n",
        "            result[key] = dict_apply(value, func)\n",
        "        else:\n",
        "            result[key] = func(value)\n",
        "    return result\n",
        "\n",
        "# with tqdm(total=max_steps, desc=\"Eval PushTStateEnv\") as pbar:\n",
        "with tqdm(total=max_steps, desc=\"Eval BlockPushLowdim\") as pbar:\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            np_obs_dict = {\n",
        "                'obs': obs.astype(np.float32)\n",
        "            }\n",
        "            # device transfer\n",
        "            obs_dict = dict_apply(np_obs_dict, \n",
        "                lambda x: torch.from_numpy(x).to(\n",
        "                device=device))\n",
        "            action_dict = policy.predict_action(obs_dict)\n",
        "        # B = 1\n",
        "        # # stack the last obs_horizon (2) number of observations\n",
        "        # obs_seq = np.stack(obs_deque)\n",
        "        # # normalize observation\n",
        "        # nobs = normalize_data(obs_seq, stats=stats['obs'])\n",
        "        # # device transfer\n",
        "        # nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
        "\n",
        "        # # infer action\n",
        "        # with torch.no_grad():\n",
        "        #     # reshape observation to (B,obs_horizon*obs_dim)\n",
        "        #     obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "        #     # initialize action from Guassian noise\n",
        "        #     noisy_action = torch.randn(\n",
        "        #         (B, pred_horizon, action_dim), device=device)\n",
        "        #     naction = noisy_action\n",
        "\n",
        "        #     # init scheduler\n",
        "        #     noise_scheduler.set_timesteps(num_diffusion_iters)\n",
        "\n",
        "        #     for k in noise_scheduler.timesteps:\n",
        "        #         # predict noise\n",
        "        #         noise_pred = ema_noise_pred_net(\n",
        "        #             sample=naction,\n",
        "        #             timestep=k,\n",
        "        #             global_cond=obs_cond\n",
        "        #         )\n",
        "\n",
        "        #         # inverse diffusion step (remove noise)\n",
        "        #         naction = noise_scheduler.step(\n",
        "        #             model_output=noise_pred,\n",
        "        #             timestep=k,\n",
        "        #             sample=naction\n",
        "        #         ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, _, info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render(mode='rgb_array'))\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))\n",
        "\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6679135008766803\n",
            "2347 3514\n",
            "0.6678998292544109\n",
            "5861\n",
            "2348 3515\n",
            "0.6679943100995732\n",
            "5863\n"
          ]
        }
      ],
      "source": [
        "m = []\n",
        "sum_n, sum_d = 0, 0\n",
        "for d in [0.6, 0.4]:\n",
        "    for s in [0.7, 0.3]:\n",
        "        if d == 0.6 and s == 0.7:\n",
        "            m = [0.7, 0.3]\n",
        "        elif d == 0.6 and s == 0.3:\n",
        "            m = [0.4, 0.6]\n",
        "        elif d == 0.4 and s == 0.7:\n",
        "            m = [0.5, 0.5]\n",
        "        elif d == 0.4 and s == 0.3:\n",
        "            m = [0.2, 0.8]\n",
        "        \n",
        "        \n",
        "        for i in range(len(m)):\n",
        "            if d == 0.6 and i == 0:\n",
        "                r = 0.3\n",
        "                a = 0.2\n",
        "                p = 0.4\n",
        "            elif d == 0.6 and i == 1:\n",
        "                r = 0.4\n",
        "                a = 0.6\n",
        "                p = 0.7\n",
        "            elif d == 0.4 and 1 == 0:\n",
        "                r = 0.5\n",
        "                a = 0.2\n",
        "                p = 0.4\n",
        "            elif d == 0.4 and i == 1:\n",
        "                r = 0.6\n",
        "                a = 0.6\n",
        "                p = 0.7\n",
        "            \n",
        "            for c in [0.5, 0.5]:\n",
        "                sum_n += d * s * m[i] * r * p * a * c\n",
        "\n",
        "for d in [0.6, 0.4]:\n",
        "    for s in [0.7, 0.3]:\n",
        "        if d == 0.6 and s == 0.7:\n",
        "            m = [0.7, 0.3]\n",
        "        elif d == 0.6 and s == 0.3:\n",
        "            m = [0.4, 0.6]\n",
        "        elif d == 0.4 and s == 0.7:\n",
        "            m = [0.5, 0.5]\n",
        "        elif d == 0.4 and s == 0.3:\n",
        "            m = [0.2, 0.8]\n",
        "        \n",
        "        \n",
        "        for i in range(len(m)):\n",
        "            if d == 0.6 and i == 0:\n",
        "                r = 0.3\n",
        "                a = 0.2\n",
        "                p = [0.6, 0.4]\n",
        "            elif d == 0.6 and i == 1:\n",
        "                r = 0.4\n",
        "                a = 0.6\n",
        "                p = [0.3, 0.7]\n",
        "            elif d == 0.4 and 1 == 0:\n",
        "                r = 0.5\n",
        "                a = 0.2\n",
        "                p = [0.6, 0.4]\n",
        "            elif d == 0.4 and i == 1:\n",
        "                r = 0.6\n",
        "                a = 0.6\n",
        "                p = [0.3, 0.7]\n",
        "            \n",
        "            for j in range(len(p)):\n",
        "                if j == 0:\n",
        "                    c = [0.7, 0.3]\n",
        "                elif j == 1:\n",
        "                    c = [0.5, 0.5]\n",
        "                for k in range(len(c)):\n",
        "                    sum_d += d * s * m[i] * r * p[j] * a * c[k]\n",
        "           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Fraction(2038, 3205)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from fractions import Fraction\n",
        "\n",
        "# Initialize variables\n",
        "sum_n, sum_d = Fraction(0), Fraction(0)\n",
        "\n",
        "# Loop through d and s values\n",
        "for d in [Fraction(6, 10), Fraction(4, 10)]:\n",
        "    for s in [Fraction(7, 10), Fraction(3, 10)]:\n",
        "        # Set m values\n",
        "        if d == Fraction(6, 10) and s == Fraction(7, 10):\n",
        "            m = [Fraction(7, 10), Fraction(3, 10)]\n",
        "        elif d == Fraction(6, 10) and s == Fraction(3, 10):\n",
        "            m = [Fraction(4, 10), Fraction(6, 10)]\n",
        "        elif d == Fraction(4, 10) and s == Fraction(7, 10):\n",
        "            m = [Fraction(5, 10), Fraction(5, 10)]\n",
        "        elif d == Fraction(4, 10) and s == Fraction(3, 10):\n",
        "            m = [Fraction(2, 10), Fraction(8, 10)]\n",
        "\n",
        "        # Iterate over m values\n",
        "        for i in range(len(m)):\n",
        "            # Set r, a, p values\n",
        "            if d == Fraction(6, 10) and i == 0:\n",
        "                r, a, p = Fraction(3, 10), Fraction(2, 10), Fraction(4, 10)\n",
        "            elif d == Fraction(6, 10) and i == 1:\n",
        "                r, a, p = Fraction(4, 10), Fraction(6, 10), Fraction(7, 10)\n",
        "            elif d == Fraction(4, 10) and i == 0:\n",
        "                r, a, p = Fraction(5, 10), Fraction(2, 10), Fraction(4, 10)\n",
        "            elif d == Fraction(4, 10) and i == 1:\n",
        "                r, a, p = Fraction(6, 10), Fraction(6, 10), Fraction(7, 10)\n",
        "\n",
        "            # Iterate over c values\n",
        "            for c in [Fraction(5, 10), Fraction(5, 10)]:\n",
        "                sum_n += d * s * m[i] * r * p * a * c\n",
        "\n",
        "# Loop again for second sum (sum_d)\n",
        "for d in [Fraction(6, 10), Fraction(4, 10)]:\n",
        "    for s in [Fraction(7, 10), Fraction(3, 10)]:\n",
        "        # Set m values\n",
        "        if d == Fraction(6, 10) and s == Fraction(7, 10):\n",
        "            m = [Fraction(7, 10), Fraction(3, 10)]\n",
        "        elif d == Fraction(6, 10) and s == Fraction(3, 10):\n",
        "            m = [Fraction(4, 10), Fraction(6, 10)]\n",
        "        elif d == Fraction(4, 10) and s == Fraction(7, 10):\n",
        "            m = [Fraction(5, 10), Fraction(5, 10)]\n",
        "        elif d == Fraction(4, 10) and s == Fraction(3, 10):\n",
        "            m = [Fraction(2, 10), Fraction(8, 10)]\n",
        "\n",
        "        # Iterate over m values\n",
        "        for i in range(len(m)):\n",
        "            if d == Fraction(6, 10) and i == 0:\n",
        "                r, a, p = Fraction(3, 10), Fraction(2, 10), [Fraction(6, 10), Fraction(4, 10)]\n",
        "            elif d == Fraction(6, 10) and i == 1:\n",
        "                r, a, p = Fraction(4, 10), Fraction(6, 10), [Fraction(3, 10), Fraction(7, 10)]\n",
        "            elif d == Fraction(4, 10) and i == 0:\n",
        "                r, a, p = Fraction(5, 10), Fraction(2, 10), [Fraction(6, 10), Fraction(4, 10)]\n",
        "            elif d == Fraction(4, 10) and i == 1:\n",
        "                r, a, p = Fraction(6, 10), Fraction(6, 10), [Fraction(3, 10), Fraction(7, 10)]\n",
        "\n",
        "            # Iterate over p and c values\n",
        "            for j in range(len(p)):\n",
        "                if j == 0:\n",
        "                    c = [Fraction(7, 10), Fraction(3, 10)]\n",
        "                elif j == 1:\n",
        "                    c = [Fraction(5, 10), Fraction(5, 10)]\n",
        "                for k in range(len(c)):\n",
        "                    sum_d += d * s * m[i] * r * p[j] * a * c[k]\n",
        "\n",
        "# Output results as fractions\n",
        "sum_n / sum_d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1167"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "3205-2038"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5243"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "2038+3205"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5243"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1167+2038*2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " \n",
        "print(sum_n / sum_d)\n",
        "for i in range(1, 100000):\n",
        "    if -0.0001 < i / (i+1167) - sum_n/sum_d < 0.0001:\n",
        "        print(i, i + 1167)\n",
        "        print(i / (i+1167))\n",
        "        print(2*i + 1167)\n",
        "\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.9922123630939464e-05\n",
            "0.999950077876369\n",
            "0.9999500778763691\n",
            "1.1862305470508222e-07\n",
            "0.0023760434084732374\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# Define parameters\n",
        "x = np.array([15, 40])\n",
        "mean = np.array([10, 30])\n",
        "covariance = np.array([[5, 0], [0, 5]])\n",
        "mean2 = np.array([18, 45])\n",
        "covariance2 = np.array([[8, 0], [0, 8]])\n",
        "\n",
        "# Calculate multivariate normal PDF\n",
        "pdf_value = multivariate_normal.pdf(x, mean=mean, cov=covariance) \\\n",
        "    / (multivariate_normal.pdf(x, mean=mean, cov=covariance) \n",
        "       + multivariate_normal.pdf(x, mean=mean2, cov=covariance2) )\n",
        "print(pdf_value)\n",
        "print(1 - pdf_value)\n",
        "pdf_value = multivariate_normal.pdf(x, mean=mean2, cov=covariance2) \\\n",
        "    / (multivariate_normal.pdf(x, mean=mean, cov=covariance) \n",
        "       + multivariate_normal.pdf(x, mean=mean2, cov=covariance2) )\n",
        "print(pdf_value)\n",
        "print(multivariate_normal.pdf(x, mean=mean, cov=covariance))\n",
        "print(multivariate_normal.pdf(x, mean=mean2, cov=covariance2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23055.262042231858"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "6 * np.log(10000) - 2*(-11500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1870821"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "64*64*425 + 425 + 425*255+255 + 255*80+80+80*6+6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1740800"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "4096*425"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "019444645b164b92a8da32e94a443d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03bbeb04a4c44206b1671e69864e69c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0964ed28f2794bdf91e2e2756aae3bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d687f5624cb4871ad167ebcbcb4c148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116ecf0deb6e44faadb594f2f982c4c2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2db415245cbb44c8ab5c208be328fc16",
            "value": 1
          }
        },
        "116ecf0deb6e44faadb594f2f982c4c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d07e24261b4627b1160beeb59c5636": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "187c8fd96f79429bb0752103d5998401": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1939a1c3cc7a498fa5f05ac737b0af99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c17844c76d84675aa1c6e1a973308c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa9489756ac24ba0b523f04a3352a09f",
              "IPY_MODEL_51ff5f151bc9475fa3a5c54f8b60a50e",
              "IPY_MODEL_58656f1d31ac45d3aaef45aa22f69899"
            ],
            "layout": "IPY_MODEL_b9f98d2d7c624a8eae13fa5b422739ce"
          }
        },
        "269772eda6cc4449bbba9e1e684b442c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2db415245cbb44c8ab5c208be328fc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36c1a61163804f9a825638c6c8d962ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a26e0dce0a86491db71a731f570f1a80",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe61754736d04d539c3f941049b5922a",
            "value": 95
          }
        },
        "3a5463e9f9864ca2b52fe9cd28f4a8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49d5ffd9eb81491f95df70e66c0c94e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a3d8fbcc3e548e1a56d25c10b321e6a",
              "IPY_MODEL_6cf470f12c174e94b0b058cfba9b5fb2",
              "IPY_MODEL_9b6de90b3dbc4877966b2d5233189866"
            ],
            "layout": "IPY_MODEL_4b3aeac3e8e74b8f8dab0dc9ad0b7c94"
          }
        },
        "4b3aeac3e8e74b8f8dab0dc9ad0b7c94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ff5f151bc9475fa3a5c54f8b60a50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187c8fd96f79429bb0752103d5998401",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17d07e24261b4627b1160beeb59c5636",
            "value": 200
          }
        },
        "5352c2178612408aa84a3732d98a62ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f451b98692b64b54a0e8a3643a9bf992",
              "IPY_MODEL_36c1a61163804f9a825638c6c8d962ed",
              "IPY_MODEL_c2d84f324ecd4789b9b3420591f6186d"
            ],
            "layout": "IPY_MODEL_6d4ec5c2f9624d398f0d9fc27d84cadb"
          }
        },
        "567e3614e80743e5ae1f8e3c75a65b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583a530a9c0442f2b762b281caa4836d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58656f1d31ac45d3aaef45aa22f69899": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269772eda6cc4449bbba9e1e684b442c",
            "placeholder": "​",
            "style": "IPY_MODEL_7cad813857df45f3ad617c74c68a619e",
            "value": " 201/? [00:37&lt;00:00,  6.07it/s, reward=0.875]"
          }
        },
        "5b6eaafac1574f7481e3bb2e20e598b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "657a261a272d466e8bfd532279a401a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf470f12c174e94b0b058cfba9b5fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d2c3f8b3714a0d911a132c74589ce1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03bbeb04a4c44206b1671e69864e69c7",
            "value": 0
          }
        },
        "6d4dfa99470e40559bebd6689305f155": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4ec5c2f9624d398f0d9fc27d84cadb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "7cad813857df45f3ad617c74c68a619e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82a5a8a6be3a41209de270d5072838a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0c7c76b76d40c9b25ca92abb00cf34",
            "placeholder": "​",
            "style": "IPY_MODEL_3a5463e9f9864ca2b52fe9cd28f4a8b8",
            "value": " 1/1 [00:32&lt;00:00, 32.94s/it, loss=0.0523]"
          }
        },
        "83bd2cb0ca534108804cdc298d8b26c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a3d8fbcc3e548e1a56d25c10b321e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac6375a33ca84241b50638edb6321112",
            "placeholder": "​",
            "style": "IPY_MODEL_583a530a9c0442f2b762b281caa4836d",
            "value": ""
          }
        },
        "9b6de90b3dbc4877966b2d5233189866": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2331069d969430b87c47e6f11fb2a9e",
            "placeholder": "​",
            "style": "IPY_MODEL_c8c282ba14c14da28ba421abc7119cf9",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "a26e0dce0a86491db71a731f570f1a80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9489756ac24ba0b523f04a3352a09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b72ff2e7020f447fb492c1e9f69a0174",
            "placeholder": "​",
            "style": "IPY_MODEL_567e3614e80743e5ae1f8e3c75a65b45",
            "value": "Eval PushTStateEnv: "
          }
        },
        "ac6375a33ca84241b50638edb6321112": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2331069d969430b87c47e6f11fb2a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72ff2e7020f447fb492c1e9f69a0174": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f98d2d7c624a8eae13fa5b422739ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c29b0972b22d4dc59844ea958abf7bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df7b2e7c39944341be1a091331fd7cc6",
              "IPY_MODEL_0d687f5624cb4871ad167ebcbcb4c148",
              "IPY_MODEL_82a5a8a6be3a41209de270d5072838a2"
            ],
            "layout": "IPY_MODEL_6d4dfa99470e40559bebd6689305f155"
          }
        },
        "c2d84f324ecd4789b9b3420591f6186d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_657a261a272d466e8bfd532279a401a1",
            "placeholder": "​",
            "style": "IPY_MODEL_5b6eaafac1574f7481e3bb2e20e598b3",
            "value": " 95/95 [00:32&lt;00:00,  3.08it/s, loss=0.0484]"
          }
        },
        "c7d2c3f8b3714a0d911a132c74589ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c8c282ba14c14da28ba421abc7119cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df7b2e7c39944341be1a091331fd7cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0964ed28f2794bdf91e2e2756aae3bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_1939a1c3cc7a498fa5f05ac737b0af99",
            "value": "Epoch: 100%"
          }
        },
        "ed0c7c76b76d40c9b25ca92abb00cf34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f451b98692b64b54a0e8a3643a9bf992": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019444645b164b92a8da32e94a443d7e",
            "placeholder": "​",
            "style": "IPY_MODEL_83bd2cb0ca534108804cdc298d8b26c6",
            "value": "Batch: 100%"
          }
        },
        "fe61754736d04d539c3f941049b5922a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
